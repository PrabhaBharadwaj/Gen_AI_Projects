{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Vector DB - PINECONE DB**\n","\n","**Note - This script executed in Google Colab**\n","\n","\n","\n","- **Pinecone is a cloud based** Vector DB.\n","- **Pinecone and Weaviate**  are **cloud based db**, we need to **take subscription**, but it provides initial free credits, we can create only 1 cluster. if we **dont want to save our private data** there, then we should use Chrome db/FAISS\n","- Set **API key** in Pinecone website\n","- We need to **define/create index** and its **dimensions**. That time we get **API_Env**.\n","\t- **PINECONE_API_ENV = 'gcp-starter'**\n","- If our embedding model creates vector of 384 diemnsions then we need to set diemnsions =384, while creating pinecone index. Then **.init** initialize the pinecone by providing index\n","\n","- Whenever **connecting to Pinecone Via  API key and env key**, that time**import direct pinecone library** and use\n","- Whenever **importing embedding and doing db registry to Pinecone**, that time use **pinecone from langchain.vectorstores import Pinecone**\n","\n","## **Terminology:**\n","- **CHROMA/PINECONE-CLIENT** Db **pip installed**, Then called via **langchain's vectore_stores**\n","- Here we used hugging faces's embedding -**sentence-transformers** - **This framework generates embeddings for each input sentence**\n","- **Chunking/Chunk_size:** In document/datset we will have more no of tokens, but word embedding LLM models will have **token size /token_limitation** like 4k Tokens etc, So to accomodate to that size, we **split our data as chunks**\n","- **Chunk_overlap =50:** It takes **50 token behind from previous chunk** while creating next chunk\n","\n","\n","## **Below steps followed:**\n","- Login to **Pinecone website(Pinecone: https://www.pinecone.io/)**, Create\n","\t- **APE_KEY**\n","\t- **API_Env**\n","\t- **New index**\n","\n","-  **Download some document**\n","- Then **split that into chunks**\n","- Then import **openai embedding or hugging face embedding model** or some other embedding which converts **tokens/text to vector**\n","- In **Pinecone** Create cluster/Index with dimention =384. Here our embedding converts chunk to **384 dimension vector**\n","- Then use **pinecone library** and pass\n","    - **document which conveted to chunks to vector**  \n","    - **embedding model name**\n","    - **index**\n","- This converts **chunk to vectors/embedding**, which will be **saved inside index in pinecone cloud**\n","- Each chunks creates as 1 vector, we can see this in **Pinecone website, under our index**\n","- Then we need to **Use this vector_db** which we just now created by mentioning **vector_db**  \n","- Then use **as_retriever** to **read vector db** and **do  symantic search on this**\n","- Then this **symantic/similarity search** will give **K=4 relavant answers**, that along **with user Q** we will **feed to LLM** to provide **meaningfull response on that Q**.\n","- We can use **langchain's chain operation** - **RetrivalQA** for this\n","- We can set this # of relevant answer by setting **search_kwargs ={k:2}**\n","by using Chroma library\n","- Here **VectorDB does similarity search based on user Q** but **LLM just structure the VectorDB response and gives as output**. LLM wont do anything else. **Its also called RAG**\n","- This **RetrievalQA** passes Q to Vector db **retriever** and then passes this O/P with Q to llm model to do **summarization** internally\n","- We can use langchain's chain operation - **RetrivalQA** or **load_qa_chain** for this\n","\n"],"metadata":{"id":"ZGcBwYetqNDZ"}},{"cell_type":"markdown","source":["Pinecone: https://www.pinecone.io/"],"metadata":{"id":"fZ4b4j8LKS_Q"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"If7W46lCJ6h6","executionInfo":{"status":"ok","timestamp":1711267348489,"user_tz":-480,"elapsed":136867,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2287f0b7-5610-4b9a-fcff-84fc2ee6f13b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain\n","  Downloading langchain-0.1.13-py3-none-any.whl (810 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n","  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n","Collecting jsonpatch<2.0,>=1.33 (from langchain)\n","  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n","Collecting langchain-community<0.1,>=0.0.29 (from langchain)\n","  Downloading langchain_community-0.0.29-py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.33 (from langchain)\n","  Downloading langchain_core-0.1.33-py3-none-any.whl (269 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n","  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n","Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n","  Downloading langsmith-0.1.31-py3-none-any.whl (71 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.4)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n","  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (3.7.1)\n","Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.33->langchain)\n","  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n","  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.3)\n","Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.33->langchain) (1.2.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: packaging, orjson, mypy-extensions, jsonpointer, typing-inspect, marshmallow, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-community, langchain\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 24.0\n","    Uninstalling packaging-24.0:\n","      Successfully uninstalled packaging-24.0\n","Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.13 langchain-community-0.0.29 langchain-core-0.1.33 langchain-text-splitters-0.0.1 langsmith-0.1.31 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 packaging-23.2 typing-inspect-0.9.0\n","Collecting pinecone-client==2.2.4\n","  Downloading pinecone_client-2.2.4-py3-none-any.whl (179 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (6.0.1)\n","Collecting loguru>=0.5.0 (from pinecone-client==2.2.4)\n","  Downloading loguru-0.7.2-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (4.10.0)\n","Collecting dnspython>=2.0.0 (from pinecone-client==2.2.4)\n","  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.8.2)\n","Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (2.0.7)\n","Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (4.66.2)\n","Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client==2.2.4) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone-client==2.2.4) (1.16.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (3.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pinecone-client==2.2.4) (2024.2.2)\n","Installing collected packages: loguru, dnspython, pinecone-client\n","Successfully installed dnspython-2.6.1 loguru-0.7.2 pinecone-client-2.2.4\n","Collecting pypdf\n","  Downloading pypdf-4.1.0-py3-none-any.whl (286 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.1/286.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypdf\n","Successfully installed pypdf-4.1.0\n","Collecting sentence-transformers==2.2.2\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.38.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (4.66.2)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (2.2.1+cu121)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.17.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.25.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (3.8.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.1.99)\n","Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers==2.2.2) (0.20.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.13.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.31.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (23.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.3)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers==2.2.2)\n","  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.4.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence-transformers==2.2.2) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.3.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence-transformers==2.2.2) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\n","Building wheels for collected packages: sentence-transformers\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=c1fc97334df2a0b1ccd996c5450e148340b3ff3be2c0e3edec48bc5265e9e765\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence-transformers\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.2.2\n"]}],"source":["!pip install langchain\n","!pip install pinecone-client==2.2.4\n","!pip install pypdf\n","!pip install sentence-transformers==2.2.2"]},{"cell_type":"markdown","source":["## **1. Read the Document**\n","- Create directory pdfs and keep pdf file here, which will be used to created DB\n","- This pdf folder creating inside colab env,so it will deleted once session completes"],"metadata":{"id":"OtiuaOcKw0ZM"}},{"cell_type":"code","source":["!mkdir pdfs"],"metadata":{"id":"mdOSMa98MNGs","executionInfo":{"status":"ok","timestamp":1711267193770,"user_tz":-480,"elapsed":16,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!ls -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cmyVe1cPyqUj","executionInfo":{"status":"ok","timestamp":1711267194068,"user_tz":-480,"elapsed":310,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"a26eeb84-a945-4901-b64a-1267ef885f3d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["total 8\n","drwxr-xr-x 2 root root 4096 Mar 24 07:59 pdfs\n","drwxr-xr-x 1 root root 4096 Mar 21 13:23 sample_data\n"]}]},{"cell_type":"markdown","source":["### **Extract the Text from the PDF's**"],"metadata":{"id":"8PlnBWxDNN7l"}},{"cell_type":"code","source":["from langchain.document_loaders import PyPDFDirectoryLoader"],"metadata":{"id":"KKF5G9mLNSe4","executionInfo":{"status":"ok","timestamp":1711267349276,"user_tz":-480,"elapsed":797,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["loader = PyPDFDirectoryLoader(\"pdfs\")\n","data = loader.load()\n","data[:1]"],"metadata":{"id":"wptoFDrKMcD8","executionInfo":{"status":"ok","timestamp":1711267376129,"user_tz":-480,"elapsed":1205,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f3c552c4-581a-4d72-9a43-151e37179658"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='Retrieval-Augmented Generation for Large Language Models: A Survey\\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\\nDai1,Jiawei Sun1,Qianyu Guo4,Meng Wang3and Haofen Wang1,3∗\\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n3College of Design and Innovation, Tongji University\\n4School of Computer Science, Fudan University\\nAbstract\\nLarge Language Models (LLMs) demonstrate\\nsignificant capabilities but face challenges such\\nas hallucination, outdated knowledge, and non-\\ntransparent, untraceable reasoning processes.\\nRetrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating\\nknowledge from external databases. This enhances\\nthe accuracy and credibility of the models, particu-\\nlarly for knowledge-intensive tasks, and allows for\\ncontinuous knowledge updates and integration of\\ndomain-specific information. RAG synergistically\\nmerges LLMs’ intrinsic knowledge with the vast,\\ndynamic repositories of external databases. This\\ncomprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms,\\nencompassing the Naive RAG, the Advanced RAG,\\nand the Modular RAG. It meticulously scrutinizes\\nthe tripartite foundation of RAG frameworks,\\nwhich includes the retrieval , the generation and\\nthe augmentation techniques. The paper highlights\\nthe state-of-the-art technologies embedded in\\neach of these critical components, providing a\\nprofound understanding of the advancements in\\nRAG systems. Furthermore, this paper introduces\\nthe metrics and benchmarks for assessing RAG\\nmodels, along with the most up-to-date evaluation\\nframework. In conclusion, the paper delineates\\nprospective avenues for research, including the\\nidentification of challenges, the expansion of\\nmulti-modalities, and the progression of the RAG\\ninfrastructure and its ecosystem.1.\\n1 Introduction\\nLarge language models (LLMs) such as the GPT se-\\nries[Brown et al. , 2020, OpenAI, 2023 ]and the LLama se-\\nries [Touvron et al. , 2023 ], along with other models like\\nGemini [Google, 2023 ], have achieved remarkable suc-\\ncess in natural language processing, demonstrating supe-\\n∗Corresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Surveyrior performance on various benchmarks including Super-\\nGLUE [Wang et al. , 2019 ], MMLU [Hendrycks et al. , 2020 ],\\nand BIG-bench [Srivastava et al. , 2022 ]. Despite these\\nadvancements, LLMs exhibit notable limitations, par-\\nticularly in handling domain-specific or highly special-\\nized queries [Kandpal et al. , 2023 ]. A common issue is\\nthe generation of incorrect information, or ”hallucina-\\ntions” [Zhang et al. , 2023b ], especially when queries extend\\nbeyond the model’s training data or necessitate up-to-date in-\\nformation. These shortcomings underscore the impractical-\\nity of deploying LLMs as black-box solutions in real-world\\nproduction environments without additional safeguards. One\\npromising approach to mitigate these limitations is Retrieval-\\nAugmented Generation (RAG), which integrates external\\ndata retrieval into the generative process, thereby enhancing\\nthe model’s ability to provide accurate and relevant responses.\\nRAG, introduced by Lewis et al. [Lewis et al. , 2020 ]in\\nmid-2020, stands as a paradigm within the realm of LLMs,\\nenhancing generative tasks. Specifically, RAG involves an\\ninitial retrieval step where the LLMs query an external data\\nsource to obtain relevant information before proceeding to an-\\nswer questions or generate text. This process not only informs\\nthe subsequent generation phase but also ensures that the re-\\nsponses are grounded in retrieved evidence, thereby signif-\\nicantly enhancing the accuracy and relevance of the output.\\nThe dynamic retrieval of information from knowledge bases\\nduring the inference phase allows RAG to address issues such\\nas the generation of factually incorrect content, commonly\\nreferred to as “hallucinations.” The integration of RAG into\\nLLMs has seen rapid adoption and has become a pivotal tech-\\nnology in refining the capabilities of chatbots and rendering\\nLLMs more viable for practical applications.\\nThe evolutionary trajectory of RAG unfolds across four\\ndistinctive phases, as illustrated in Figure 1. In its in-\\nception in 2017, aligned with the emergence of the Trans-\\nformer architecture, the primary thrust was on assimilating\\nadditional knowledge through Pre-Training Models (PTM)\\nto augment language models. This epoch witnessed RAG’s\\nfoundational efforts predominantly directed at optimizing\\npre-training methodologies.\\nFollowing this initial phase, a period of relative dormancy\\nensued before the advent of chatGPT, during which there was\\nminimal advancement in related research for RAG. The sub-\\nsequent arrival of chatGPT marked a pivotal moment in thearXiv:2312.10997v4  [cs.CL]  5 Jan 2024', metadata={'source': 'pdfs/RAG_LLM_Pdf.pdf', 'page': 0})]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### Split the whole document to chunks\n","- split that into chunks with **chunk_size=500, chunk_overlap=20** using **RecursiveCharacterTextSplitter**"],"metadata":{"id":"KGYUaj3eNykP"}},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n","text_chunks = text_splitter.split_documents(data)"],"metadata":{"id":"bXHGpiZ5NVHW","executionInfo":{"status":"ok","timestamp":1711267556778,"user_tz":-480,"elapsed":462,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["len(text_chunks) # Total it creats 281 chunks based on specified chunk size and chunk_overlap"],"metadata":{"id":"XV2CrWAINnE1","executionInfo":{"status":"ok","timestamp":1711267562217,"user_tz":-480,"elapsed":288,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec9ae2cf-d7ae-4b96-e58b-7e395cd8a715"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["281"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["text_chunks[2]"],"metadata":{"id":"IbwqGwRYNnCU","executionInfo":{"status":"ok","timestamp":1711267568390,"user_tz":-480,"elapsed":284,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"da11eca5-c4ea-436f-e15f-91df25f9f4ae"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Document(page_content='domain-specific information. RAG synergistically\\nmerges LLMs’ intrinsic knowledge with the vast,\\ndynamic repositories of external databases. This\\ncomprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms,\\nencompassing the Naive RAG, the Advanced RAG,\\nand the Modular RAG. It meticulously scrutinizes\\nthe tripartite foundation of RAG frameworks,\\nwhich includes the retrieval , the generation and\\nthe augmentation techniques. The paper highlights', metadata={'source': 'pdfs/RAG_LLM_Pdf.pdf', 'page': 0})"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## **2. Creating Vector DB**\n","\n","- Then import **openai embedding or hugging face embedding model** or some other embedding which converts **tokens/text to vector**\n","- In **Pinecone** Create cluster/Index with dimention =384. Here our embedding converts chunk to **384 dimension vector**\n","- Then use **Pinecone/vectore db library** and pass\n","    - **document which conveted to chunks to vector**  \n","    - **embedding model name**\n","    - **index**\n","- This converts **chunk to vectors/embedding**, which will be **saved inside index in pinecone cloud**"],"metadata":{"id":"tIY40r0rN00u"}},{"cell_type":"markdown","source":["### **Initialize Embedding**\n","\n","- Used Hugging face embedding - **sentence-transformers/all-MiniLM-L6-v2**\n","- Here it downloads embedding model"],"metadata":{"id":"-sc631VHzZMy"}},{"cell_type":"code","source":["from langchain.embeddings import HuggingFaceEmbeddings\n","#from langchain.embeddings import OpenAIEmbeddings\n","\n","embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"],"metadata":{"id":"UIAnc8_WNsp8","executionInfo":{"status":"ok","timestamp":1711267579184,"user_tz":-480,"elapsed":299,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Test this initialized embedding model with sample text\n","query_result = embeddings.embed_query(\"Hello World\")\n","query_result[:10]"],"metadata":{"id":"g9U0vZ1GOOhE","executionInfo":{"status":"ok","timestamp":1711267586131,"user_tz":-480,"elapsed":1904,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"eea3b1f7-6e90-45b2-ade1-3296b6b0b57a"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[-0.03447727486491203,\n"," 0.03102317824959755,\n"," 0.006734995171427727,\n"," 0.026108944788575172,\n"," -0.039361994713544846,\n"," -0.16030240058898926,\n"," 0.06692399084568024,\n"," -0.006441427860409021,\n"," -0.04745052009820938,\n"," 0.014758813194930553]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["print(\"Length\", len(query_result))"],"metadata":{"id":"z4S3r-RGOYEk","executionInfo":{"status":"ok","timestamp":1711267589073,"user_tz":-480,"elapsed":264,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e4dd03d-eef5-45e1-f28e-30b633a30d3a"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Length 384\n"]}]},{"cell_type":"markdown","source":["> This embedding model creates vector with **384 dimension**, so we created pinecone index also 384 dimension"],"metadata":{"id":"zVmU84Yx0QzF"}},{"cell_type":"markdown","source":["### **Intialize Pinecone Vector DB**"],"metadata":{"id":"kjDDGZ1IOd10"}},{"cell_type":"markdown","source":["#### Invoke and Initialize Pinecone"],"metadata":{"id":"3P95CYM32P96"}},{"cell_type":"code","source":["from google.colab import userdata\n","PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n","PINECONE_API_ENV = userdata.get('PINECONE_API_ENV')\n","\n","import os\n","#Make is as env variable\n","os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n","os.environ[\"PINECONE_API_ENV\"] = PINECONE_API_ENV"],"metadata":{"id":"gHHNhKtaOinp","executionInfo":{"status":"ok","timestamp":1711267594934,"user_tz":-480,"elapsed":2441,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["import pinecone\n","\n","# initialize pinecone\n","pinecone.init(\n","    api_key=PINECONE_API_KEY,  # find at app.pinecone.io\n","    environment=PINECONE_API_ENV  # next to api key in console\n",")\n","index_name = \"testindex\" # put in the name of your pinecone index here\n"],"metadata":{"id":"tLBDqUA5PDq9","executionInfo":{"status":"ok","timestamp":1711267644390,"user_tz":-480,"elapsed":269,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":["### **Create Vector DB**\n","- Then use **pinecone library** and pass\n","    - **document which conveted to chunks to vector**  \n","    - **embedding model name**\n","    - **index**"],"metadata":{"id":"kuwdatvd3ZMv"}},{"cell_type":"code","source":["from langchain.vectorstores import Pinecone\n","\n","#Initialize Pinecone by passing text which converted as chunks, embedding model and schema name\n","docsearch = Pinecone.from_texts([t.page_content for t in text_chunks],\n","                                embeddings,\n","                                index_name=index_name)"],"metadata":{"id":"xA40p1jMPMx8","executionInfo":{"status":"ok","timestamp":1711267678188,"user_tz":-480,"elapsed":30739,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["### Load the Vector DB from Pinecone\n","- load this vector_db which we just now created by mentioning index_name and embedding model name\n","- If you already have an index(Means already have existing PINECONE Vector DB index with all vector data), you can load it like this\n","\n","- docsearch = Pinecone.from_existing_index(index_name, embeddings)"],"metadata":{"id":"aMMqIPGLP8hH"}},{"cell_type":"code","source":["docsearch = Pinecone.from_existing_index(index_name, embeddings) # This step used if we are calling already existing index\n","docsearch"],"metadata":{"id":"abeq0PEmPfiu","executionInfo":{"status":"ok","timestamp":1711267751146,"user_tz":-480,"elapsed":1574,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"151d9cfb-7d00-4e73-8248-0fc7bb578d99"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<langchain_community.vectorstores.pinecone.Pinecone at 0x7da7786a2a10>"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["## **3. Sementic/Similarity Search**\n","- Then use **similarity_search** to read vector db and do **symantic search** on this\n","- Then this symantic/similarity search will give K=4 relavant answers, that along with user Q we will feed to LLM to provide meaningfull response on that Q.\n","We can use langchain's chain operation - RetrivalQA for this"],"metadata":{"id":"6rf0vT0TQHD9"}},{"cell_type":"markdown","source":["### Set Retrival argument **search_kwargs={\"k\": 3}**"],"metadata":{"id":"usL1z9Cn4pkN"}},{"cell_type":"code","source":["query = \"What is yolo?\""],"metadata":{"id":"2QPqDP4uPfgG","executionInfo":{"status":"ok","timestamp":1711267754208,"user_tz":-480,"elapsed":267,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["docs = docsearch.similarity_search(query, k=3)\n","docs"],"metadata":{"id":"SdTGw7HsPfd8","executionInfo":{"status":"ok","timestamp":1711267759138,"user_tz":-480,"elapsed":2221,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b6ad513-447e-4638-9432-e7405da52e7a"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\\ndoes think one person is an airplane.\\nincluding the time to fetch images from the camera and dis-\\nplay the detections.\\nThe resulting system is interactive and engaging. While\\nYOLO processes images individually, when attached to a\\nwebcam it functions like a tracking system, detecting ob-\\njects as they move around and change in appearance. A'),\n"," Document(page_content='Figure 6: Qualitative Results. YOLO running on sample artwork and natural images from the internet. It is mostly accurate although it\\ndoes think one person is an airplane.\\nincluding the time to fetch images from the camera and dis-\\nplay the detections.\\nThe resulting system is interactive and engaging. While\\nYOLO processes images individually, when attached to a\\nwebcam it functions like a tracking system, detecting ob-\\njects as they move around and change in appearance. A'),\n"," Document(page_content='making predictions. Unlike sliding window and region\\nproposal-based techniques, YOLO sees the entire image\\nduring training and test time so it implicitly encodes contex-\\ntual information about classes as well as their appearance.\\nFast R-CNN, a top detection method [14], mistakes back-\\nground patches in an image for objects because it can’t see\\nthe larger context. YOLO makes less than half the number\\nof background errors compared to Fast R-CNN.')]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["len(docs)"],"metadata":{"id":"TWp8rA1-QSWd","executionInfo":{"status":"ok","timestamp":1711267764601,"user_tz":-480,"elapsed":264,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3116f2d3-5412-45a2-88b3-1736c207fbd9"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["## **4. Use OPENAI LLM Model and Make a chain and do Semantic Search**\n","- We can use **langchain's chain operation** - **RetrivalQA** for this\n","- We can set this # of relevant answer by setting search_kwargs ={k:2} by using Chroma library\n","- Here VectorDB does **similarity search** based on **user Q** but **LLM just structure the VectorDB response and gives as output**. LLM wont do anything else. Its also called RAG\n","- This **RetrievalQA** passes Q to Vector db **retriever** and then passes this O/P with Q to llm model to do **summarization** internally\n","- We can use langchain's chain operation - **RetrivalQA** or **load_qa_chain** for this"],"metadata":{"id":"syoG7NDU4yMh"}},{"cell_type":"code","source":["!pip install openai -q"],"metadata":{"id":"FqnA7aNTQqx2","executionInfo":{"status":"ok","timestamp":1711267854601,"user_tz":-480,"elapsed":7698,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n","\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"],"metadata":{"id":"YJ5-RaMYQin9","executionInfo":{"status":"ok","timestamp":1711267782372,"user_tz":-480,"elapsed":1234,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["from langchain.llms import OpenAI\n","from langchain.chains import RetrievalQA"],"metadata":{"id":"OefT4CPyQcVC","executionInfo":{"status":"ok","timestamp":1711267787426,"user_tz":-480,"elapsed":441,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["llm = OpenAI()"],"metadata":{"id":"MCgk1uIUQT1e","executionInfo":{"status":"ok","timestamp":1711267792756,"user_tz":-480,"elapsed":2907,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9ac4aa1c-3c31-4bfa-e255-4446e28c63b2"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n","  warn_deprecated(\n"]}]},{"cell_type":"code","source":["qa = RetrievalQA.from_chain_type(llm=llm,\n","                                 chain_type=\"stuff\",\n","                                 retriever=docsearch.as_retriever()\n","                                )"],"metadata":{"id":"lhFq_WGjQoCe","executionInfo":{"status":"ok","timestamp":1711267794596,"user_tz":-480,"elapsed":459,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}}},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":["### Call Chain and get response (RAG)"],"metadata":{"id":"8Cxk0R-46CCp"}},{"cell_type":"code","source":["query = \"What is yolo?\" #which is here in content file\n","print('\\n',qa.run(query))"],"metadata":{"id":"y9FMCDRyQ5BX","executionInfo":{"status":"ok","timestamp":1711267813375,"user_tz":-480,"elapsed":3044,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a64cfde7-579d-4087-94c0-68f027f99938"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  YOLO is a detection system that is able to see the entire image during training and test time, allowing it to encode contextual information about classes and their appearance. It is also able to detect objects as they move and change in appearance, making it useful for tracking systems. Additionally, YOLO has been shown to make fewer background errors compared to other top detection methods.\n"]}]},{"cell_type":"code","source":["# full example which is not there in content file\n","query = \"what is spacex?\"\n","\n","print('\\n',qa.run(query))"],"metadata":{"id":"vpdZno-uQ9Se","executionInfo":{"status":"ok","timestamp":1711267816855,"user_tz":-480,"elapsed":1415,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a087f96-4ecb-48b7-fbd6-0f1415292dd8"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","  I don't know, as there is no mention of SpaceX in the given context.\n"]}]},{"cell_type":"markdown","source":["# **END**"],"metadata":{"id":"-RdyrEqx6SAh"}}]}