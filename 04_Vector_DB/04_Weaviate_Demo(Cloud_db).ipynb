{"cells":[{"cell_type":"markdown","metadata":{"id":"JO0ACyIzCOTe"},"source":["# **Vector DB - Weaviate DB**\n","\n","**Note - This script executed in Google Colab**\n","\n","- https://console.weaviate.cloud/\n","\n","\n","- **Weaviate is a cloud based** Vector DB. It allows **inmemory storage also**\n","- **Pinecone and Weaviate**  are **cloud based db**, we need to **take subscription**, but it provides initial free credits, we can create only 1 cluster. if we **dont want to save our private data** there, then we should use Chrome db/FAISS\n","- Set **API key** in Pinecone website\n","- We need to **define/create Cluster** . That time we get **WEAVIATE_CLUSTER**.\n","\t- **WEAVIATE_CLUSTER = 'xxxx'**\n","- **Pinecone** is more fast and good than **Weaviate**. But both are good.\n","\n","- Whenever **connecting to Weaviate Via  API key and env key**, that time**import direct Weaviate library** and use\n","- Whenever **importing embedding and doing db registry to Weaviate**, that time use **Weaviate from langchain.vectorstores import Weaviate**\n","\n","## **Terminology:**\n","- **CHROMA/PINECONE-CLIENT** Db **pip installed**, Then called via **langchain's vectore_stores**\n","- Here we used hugging faces's embedding -**sentence-transformers** - **This framework generates embeddings for each input sentence**\n","- **Chunking/Chunk_size:** In document/datset we will have more no of tokens, but word embedding LLM models will have **token size /token_limitation** like 4k Tokens etc, So to accomodate to that size, we **split our data as chunks**\n","- **Chunk_overlap =50:** It takes **50 token behind from previous chunk** while creating next chunk\n","\n","\n","## **Below steps followed:**\n","- Login to **Pinecone website(Pinecone: https://www.pinecone.io/)**, Create\n","\t- **APE_KEY**\n","\t- **API_Env**\n","\t- **New index**\n","\n","-  **Download some document**\n","- Then **split that into chunks**\n","- Then import **openai embedding or hugging face embedding model** or some other embedding which converts **tokens/text to vector**\n","- Then use **pinecone library** and pass\n","    - **document which conveted to chunks to vector**  \n","    - **embedding model name**\n","    - **index**\n","- This converts **chunk to vectors/embedding**, which will be **saved inside index in pinecone cloud**\n","- Each chunks creates as 1 vector, we can see this in **Pinecone website, under our index**\n","- Then we need to **Use this vector_db** which we just now created by mentioning **vector_db**  \n","- Then use **as_retriever** to **read vector db** and **do  symantic search on this**\n","- Then this **symantic/similarity search** will give **K=4 relavant answers**, that along **with user Q** we will **feed to LLM** to provide **meaningfull response on that Q**.\n","- We can use **langchain's chain operation** - **RetrivalQA** for this\n","- We can set this # of relevant answer by setting **search_kwargs ={k:2}**\n","by using Chroma library\n","- Here **VectorDB does similarity search based on user Q** but **LLM just structure the VectorDB response and gives as output**. LLM wont do anything else. **Its also called RAG**\n","- This **RetrievalQA** passes Q to Vector db **retriever** and then passes this O/P with Q to llm model to do **summarization** internally\n","- We can use langchain's chain operation - **RetrivalQA** or **load_qa_chain** for this\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcR2ogx3UNWm"},"outputs":[],"source":["!pip install weaviate-client langchain openai pypdf -q"]},{"cell_type":"markdown","metadata":{"id":"WVPRn6GAFpEU"},"source":["## **0.  Use OPENAIKEY and WEAVIATE Key**\n","- Use OPENAIKEY and WEAVIATE Key to connect OpenAI and Weaviate separately"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AjYAduCwc6Q"},"outputs":[],"source":["from google.colab import userdata\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n","\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIvw4-3swjbt"},"outputs":[],"source":["WEAVIATE_API_KEY = userdata.get('WEAVIATE_API_KEY')\n","WEAVIATE_CLUSTER = userdata.get('WEAVIATE_CLUSTER')\n","\n","import os\n","#Make is as env variable\n","os.environ[\"WEAVIATE_API_KEY\"] = WEAVIATE_API_KEY\n","os.environ[\"WEAVIATE_CLUSTER\"] = WEAVIATE_CLUSTER"]},{"cell_type":"markdown","metadata":{"id":"g1cyA4LVYFXP"},"source":["## **1. Read the Document**\n","- Create directory pdfs and keep pdf file here, which will be used to created DB\n","- This pdf folder creating inside colab env,so it will deleted once session completes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":601,"status":"ok","timestamp":1710994162581,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"},"user_tz":-480},"id":"Wu9o2myHzHZh","outputId":"f79fc3ec-f7c7-4641-8c49-a2e24611ae57"},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘data’: File exists\n"]}],"source":["!mkdir data"]},{"cell_type":"markdown","metadata":{"id":"hJu0Fv9WHDjK"},"source":["### **Extract the Text from the PDF's**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1009,"status":"ok","timestamp":1710994163585,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"},"user_tz":-480},"id":"To4_IwCGYEW4","outputId":"b9fbb209-c664-4d76-d485-80f7608ea19d"},"outputs":[{"data":{"text/plain":["[Document(page_content='Retrieval-Augmented Generation for Large Language Models: A Survey\\nYunfan Gao1,Yun Xiong2,Xinyu Gao2,Kangxiang Jia2,Jinliu Pan2,Yuxi Bi3,Yi\\nDai1,Jiawei Sun1,Qianyu Guo4,Meng Wang3and Haofen Wang1,3∗\\n1Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\n2Shanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n3College of Design and Innovation, Tongji University\\n4School of Computer Science, Fudan University\\nAbstract\\nLarge Language Models (LLMs) demonstrate\\nsignificant capabilities but face challenges such\\nas hallucination, outdated knowledge, and non-\\ntransparent, untraceable reasoning processes.\\nRetrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating\\nknowledge from external databases. This enhances\\nthe accuracy and credibility of the models, particu-\\nlarly for knowledge-intensive tasks, and allows for\\ncontinuous knowledge updates and integration of\\ndomain-specific information. RAG synergistically\\nmerges LLMs’ intrinsic knowledge with the vast,\\ndynamic repositories of external databases. This\\ncomprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms,\\nencompassing the Naive RAG, the Advanced RAG,\\nand the Modular RAG. It meticulously scrutinizes\\nthe tripartite foundation of RAG frameworks,\\nwhich includes the retrieval , the generation and\\nthe augmentation techniques. The paper highlights\\nthe state-of-the-art technologies embedded in\\neach of these critical components, providing a\\nprofound understanding of the advancements in\\nRAG systems. Furthermore, this paper introduces\\nthe metrics and benchmarks for assessing RAG\\nmodels, along with the most up-to-date evaluation\\nframework. In conclusion, the paper delineates\\nprospective avenues for research, including the\\nidentification of challenges, the expansion of\\nmulti-modalities, and the progression of the RAG\\ninfrastructure and its ecosystem.1.\\n1 Introduction\\nLarge language models (LLMs) such as the GPT se-\\nries[Brown et al. , 2020, OpenAI, 2023 ]and the LLama se-\\nries [Touvron et al. , 2023 ], along with other models like\\nGemini [Google, 2023 ], have achieved remarkable suc-\\ncess in natural language processing, demonstrating supe-\\n∗Corresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources are available at https://github.com/Tongji-KGLLM/\\nRAG-Surveyrior performance on various benchmarks including Super-\\nGLUE [Wang et al. , 2019 ], MMLU [Hendrycks et al. , 2020 ],\\nand BIG-bench [Srivastava et al. , 2022 ]. Despite these\\nadvancements, LLMs exhibit notable limitations, par-\\nticularly in handling domain-specific or highly special-\\nized queries [Kandpal et al. , 2023 ]. A common issue is\\nthe generation of incorrect information, or ”hallucina-\\ntions” [Zhang et al. , 2023b ], especially when queries extend\\nbeyond the model’s training data or necessitate up-to-date in-\\nformation. These shortcomings underscore the impractical-\\nity of deploying LLMs as black-box solutions in real-world\\nproduction environments without additional safeguards. One\\npromising approach to mitigate these limitations is Retrieval-\\nAugmented Generation (RAG), which integrates external\\ndata retrieval into the generative process, thereby enhancing\\nthe model’s ability to provide accurate and relevant responses.\\nRAG, introduced by Lewis et al. [Lewis et al. , 2020 ]in\\nmid-2020, stands as a paradigm within the realm of LLMs,\\nenhancing generative tasks. Specifically, RAG involves an\\ninitial retrieval step where the LLMs query an external data\\nsource to obtain relevant information before proceeding to an-\\nswer questions or generate text. This process not only informs\\nthe subsequent generation phase but also ensures that the re-\\nsponses are grounded in retrieved evidence, thereby signif-\\nicantly enhancing the accuracy and relevance of the output.\\nThe dynamic retrieval of information from knowledge bases\\nduring the inference phase allows RAG to address issues such\\nas the generation of factually incorrect content, commonly\\nreferred to as “hallucinations.” The integration of RAG into\\nLLMs has seen rapid adoption and has become a pivotal tech-\\nnology in refining the capabilities of chatbots and rendering\\nLLMs more viable for practical applications.\\nThe evolutionary trajectory of RAG unfolds across four\\ndistinctive phases, as illustrated in Figure 1. In its in-\\nception in 2017, aligned with the emergence of the Trans-\\nformer architecture, the primary thrust was on assimilating\\nadditional knowledge through Pre-Training Models (PTM)\\nto augment language models. This epoch witnessed RAG’s\\nfoundational efforts predominantly directed at optimizing\\npre-training methodologies.\\nFollowing this initial phase, a period of relative dormancy\\nensued before the advent of chatGPT, during which there was\\nminimal advancement in related research for RAG. The sub-\\nsequent arrival of chatGPT marked a pivotal moment in thearXiv:2312.10997v4  [cs.CL]  5 Jan 2024', metadata={'source': 'data/RAG_LLM_Pdf.pdf', 'page': 0})]"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["from langchain.document_loaders import PyPDFDirectoryLoader\n","\n","loader = PyPDFDirectoryLoader(\"data\")\n","data = loader.load()\n","data[:1]"]},{"cell_type":"markdown","metadata":{"id":"r9yWlFvXZGig"},"source":["### Split the whole document to chunks\n","- split that into chunks with **chunk_size=500, chunk_overlap=20** using **RecursiveCharacterTextSplitter**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JiGn3GNuZFtn"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","\n","text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n","docs = text_splitter.split_documents(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1710994163586,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"},"user_tz":-480},"id":"jOHRyXobaF1P","outputId":"d93521ba-d70a-4614-b9dc-7efd53c73490"},"outputs":[{"data":{"text/plain":["140"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["len(docs)"]},{"cell_type":"markdown","metadata":{"id":"LrolWA93aXAv"},"source":["## **2. Creating Vector DB**\n","\n","- Then import **openai embedding or hugging face embedding model** or some other embedding which converts **tokens/text to vector**\n","- Then use **Chroma/vectore db library** and pass\n","    - **document which conveted to chunks to vector**  \n","    - **embedding model name**\n","    - **persist directory**\n","- This converts **chunk to vectors/embedding**, which will be **saved inside db folder**"]},{"cell_type":"markdown","metadata":{"id":"UjJ7b6GhHnzr"},"source":["### **Initialize Embedding**\n","\n","- Used OPENAI  embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1710994163587,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"},"user_tz":-480},"id":"n_MB0iFYaH2Y","outputId":"328b52b5-5eaf-4561-e632-cdef2eb8349d"},"outputs":[{"data":{"text/plain":["OpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x785937fe7af0>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x785937e23ee0>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","\n","embeddings = OpenAIEmbeddings()\n","\n","embeddings"]},{"cell_type":"markdown","metadata":{"id":"rPT67TZpbFFw"},"source":["### **Intialize Weaviate Vector DB**\n","\n","- Follow the documentaion on Weaviate page and use same code to connect to cloud/server"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-yIlvmKavmo"},"outputs":[],"source":["import weaviate\n","from langchain.vectorstores import Weaviate\n","\n","#Connect to weaviate Cluster\n","auth_config = weaviate.auth.AuthApiKey(api_key = WEAVIATE_API_KEY)\n","WEAVIATE_URL = WEAVIATE_CLUSTER\n","\n","client = weaviate.Client(\n","    url = WEAVIATE_URL,\n","    additional_headers = {\"X-OpenAI-Api-key\": OPENAI_API_KEY},\n","    auth_client_secret = auth_config,\n","    startup_period = 10\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":374,"status":"ok","timestamp":1710994164414,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"},"user_tz":-480},"id":"m5etuMdHcPGR","outputId":"c263f250-ce24-4fd6-9cfc-afe97c97e569"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["client.is_ready()"]},{"cell_type":"markdown","metadata":{"id":"aB1lCXlwIBqB"},"source":["### **Create Weaviate Schema**\n","- Follow the documentaion on Weaviate page and use same code to Define the structure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WEWOVbyNdNdR"},"outputs":[],"source":["# define input structure\n","client.schema.delete_all()\n","client.schema.get()\n","schema = {\n","    \"classes\": [\n","        {\n","            \"class\": \"Chatbot\",\n","            \"description\": \"Documents for chatbot\",\n","            \"vectorizer\": \"text2vec-openai\",\n","            \"moduleConfig\": {\"text2vec-openai\": {\"model\": \"ada\", \"type\": \"text\"}},\n","            \"properties\": [\n","                {\n","                    \"dataType\": [\"text\"],\n","                    \"description\": \"The content of the paragraph\",\n","                    \"moduleConfig\": {\n","                        \"text2vec-openai\": {\n","                            \"skip\": False,\n","                            \"vectorizePropertyName\": False,\n","                        }\n","                    },\n","                    \"name\": \"content\",\n","                },\n","            ],\n","        },\n","    ]\n","}\n","\n","client.schema.create(schema)\n","\n","vectorstore = Weaviate(client, \"Chatbot\", \"content\", attributes=[\"source\"])"]},{"cell_type":"markdown","metadata":{"id":"cE4YGBxCIZ5M"},"source":["### **Create Vector DB**\n","- Then use **vectorstore**  which we created schema above and pass the data (docs), Which will converts text to vector and saves in **weaviate**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqaBpQvXeNsJ"},"outputs":[],"source":["# load text into the vectorstore\n","text_meta_pair = [(doc.page_content, doc.metadata) for doc in docs]\n","texts, meta = list(zip(*text_meta_pair))\n","\n","# Pass the text as texts and meta to vectorstore, it cnverts to vector\n","vectorstore.add_texts(texts, meta)"]},{"cell_type":"markdown","metadata":{"id":"BktCnFkRe0Kq"},"source":["### Use the Loaded Vector DB and do similarity check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pLZg7Pr5edlh"},"outputs":[],"source":["query = \"what is a RAG?\"\n","\n","# retrieve text related to the query\n","docs = vectorstore.similarity_search(query, top_k=3)\n","docs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":287,"status":"ok","timestamp":1710994211786,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"},"user_tz":-480},"id":"sJq5f3vPJUUo","outputId":"2e84a48c-1162-4e65-fc28-ed52e532705f"},"outputs":[{"data":{"text/plain":["140"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["len(docs)"]},{"cell_type":"markdown","metadata":{"id":"dwNOu59BfGP4"},"source":["## **3. Sementic Search**\n","- Then use **as_retriever** to read vector db and do **symantic search** on this\n","- Then this symantic/similarity search will give K=4 relavant answers, that along with user Q we will feed to LLM to provide meaningfull response on that Q.\n","- We can use langchain's chain operation - **RetrivalQA** or **load_qa_chain** for this"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5d5qA06ifY4A"},"outputs":[],"source":["from langchain.llms import OpenAI\n","\n","from langchain.chains import RetrievalQA\n","\n","qa = RetrievalQA.from_chain_type(llm=OpenAI(),\n","                                 chain_type=\"stuff\",\n","                                 retriever=vectorstore.as_retriever()\n","                                 )\n","\n","# Another way, But this below ex is incomplete, RAG O/P not added\n","\n","\"\"\"\n","from langchain.chains.question_answering import load_qa_chain\n","# define chain\n","chain = load_qa_chain(\n","                    llm = OpenAI(),\n","                    chain_type=\"stuff\")\n","\n","# create answer\n","chain.run(input_documents=docs, question=query)\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7KuVrWoDeeF4"},"outputs":[],"source":["query = \"What is RAG?\"\n","print(\"\\n\",qa.run(query))"]},{"cell_type":"markdown","metadata":{"id":"RfqnVp0FMWuy"},"source":["# **END**"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
