{"cells":[{"cell_type":"markdown","id":"ab67b8d0","metadata":{},"source":["# **LangChain**"]},{"cell_type":"markdown","id":"qo1DQXM18bgL","metadata":{"id":"qo1DQXM18bgL"},"source":["LangChain is a framework for developing applications powered by language models.\n","\n","- GitHub: https://github.com/hwchase17/langchain\n","- Docs: https://python.langchain.com/en/latest/index.html\n","\n","- In simple **lagchain** if we give any query langchain uses its **own embedding** and converts to **vector and feeds to LLm**\n","\n","### Overview:\n","- **Installation**\n","- **Setup the environment**\n","- **LLMs**  (Connct to different LLM using API_KEY via langchain)\n","    - Tried **OPENAI** and **HuggingFace** LLM model using API_Keys\n","- **Prompt Templates** (It provides template for llm prompt)\n","    - ***prompt template** used when we have **same template multiple time**\n","    - Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM.\n","- **Chains** (Chains of prompt)\n","    - Combine LLMs and Prompts in multi-step workflows\n","    - Provided example Without Chain running one after the other\n","        - **5.1. Simple Sequential Chain**\n","            - Combine Multiple PromptTemplates\n","            - The output from the **first PromptTemplate is passed to the next PromptTemplate** as input\n","            - Only final prompt output displayed\n","        - **5.2. Sequential Chain**\n","            - Its same as **simplesequentialchain** just difference is here, we define **output_key**, so simple seq gives only last o/p , this gives both\n","            - If we want **all** the prompt output then use  **Sequential Chain**\n","\n","- **Agents** (With the help of agents, we are accessing Tools) and Tools (Tools which we are using Serpapi,wikipedia)\n","    - Whenever we want **real time data** which is not there in LLM(Which trained using old data), then only we use Agents. If data is fixed then we use RAG\n","    - Agent will conenct with **external tools** and it will use LLM reasoning capabilities\n","    - All the tools like **Google Search Tool and Math Tool are available as part of LangChain and you can configure  agent, so agent is nothing but using all these tools and LLM reasoning capabilities** to perform a given task \n","    - To access Google Search Results in Real Time we use **serpapi**\n","\n","        - **Tool:** A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n","        - **LLM:** The language model powering the agent.\n","        - **Agent:** The agent to use.\n","\n","    - **6.1 AGENT: serpapi(Google serach) and llm-math tool**\n","    - **6.2 AGENT: Wikipedia and llm-math tool**\n","         - **Wikipedia is open source api**, so while reading this we doesn't mention any API KEY,  BUT **Serpapi** key used for  **Google search**  just **load_tool name will change from serpapi or wikipedia**\n","    - **6.3 AGENT: Human as a tool**\n","        - Here it prompts **Human**, for ans\n","\n","- **Memory** (Remembering Chat History)\n","    - We can attach memory to remember all previous conversation\n","    - **Conversation chain** and **Conversation buffer memory** are same only, It keep on adds content/ q&a respnse to memory, keeps growing\n","\n","    - **7.1 ConversationBufferMemory**\n","        - We can attach memory to remember all previous conversation\n","Conversation buffer memory goes growing endlessly, It takes huge space\n","\n","    - **7.2 ConversationChain**\n","\n","    - This is same like Conversation buffer memory, It keep on adds content/ q&a respnse to memory, keeps growing\n","    - It behaves like AI and human conversation\n","\n","\n","    - **7.3 ConversationBufferWindowMemory**\n","\n","    - Conversation buffer memory and ConversationChain goes growing endlessly, It takes huge space\n","    - It  Just remember last 5 Conversation Chain based on we set parameter k=5\n","    - It Just remember last 10-20 Conversation Chain based on we set parameter k=10/20\n","\n","- **Document Loaders** (Load our PDF file)\n","    - Langchain **Document Loader** converts any type(.pdf,.csv .json etc) format data and converts as **document**\n","    - Load the pdf file via langchain framework\n","    - https://python.langchain.com/docs/modules/data_connection/document_loaders/\n","    - By using this **Document loader** we can read any format document and then split into **chunks** and use **embedding** to convert to vector and save in **vector db** then do **retrival(RAG)** operation\n","\n","- **Indexes**"]},{"cell_type":"markdown","id":"09CgA1RZkiC4","metadata":{"id":"09CgA1RZkiC4"},"source":["## **01: Installation**\n","#!pip install langchain"]},{"cell_type":"markdown","id":"sQHZiF38-Cps","metadata":{"id":"sQHZiF38-Cps"},"source":["## **02: Setup the Environment**"]},{"cell_type":"code","execution_count":2,"id":"9-mFf0Ql-KX2","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709951875550,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"9-mFf0Ql-KX2"},"outputs":[],"source":["import os"]},{"cell_type":"markdown","id":"b2f89528","metadata":{},"source":["### Set OPENAI_API_KEY and HUGGINGFACEHUB_API_TOKEN as environmental variable Instead of reading from .env\n","\n","os.environ['OPENAI_API_KEY'] = \"Type Openapi key\"\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Type hugging face key \"\n","\n","#Or else try below method, read from .env file"]},{"cell_type":"code","execution_count":3,"id":"f31c4cc6","metadata":{"executionInfo":{"elapsed":604,"status":"ok","timestamp":1709951917175,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"f31c4cc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["sk-JVQrSfp\n"]},{"data":{"text/plain":["'hf_HYYUqMs'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# This is to read OPEN API KEYS from .env file which we created\n","from dotenv import load_dotenv\n","load_dotenv()  # take environment variables from .env.\n","\n","OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")  \n","print(OPENAI_API_KEY[:10])\n","\n","HUGGINGFACE_TOKEN=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n","HUGGINGFACE_TOKEN[:10]"]},{"cell_type":"markdown","id":"9ed0dc6a","metadata":{"id":"9ed0dc6a"},"source":["## **03: Large Language Models**"]},{"cell_type":"markdown","id":"516GZwvpnVpV","metadata":{"id":"516GZwvpnVpV"},"source":["The basic building block of LangChain is a Large Language Model which takes text as input and generates more text"]},{"cell_type":"markdown","id":"4FDyNMY3sRMc","metadata":{"id":"4FDyNMY3sRMc"},"source":["Suppose we want to generate a company name based on the company description, so we will first initialize an OpenAI wrapper. In this case, since we want the output to be more random, we will intialize our model with high temprature."]},{"cell_type":"markdown","id":"eLqFwlXaH8f4","metadata":{"id":"eLqFwlXaH8f4"},"source":["The temperature parameter adjusts the randomness of the output. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic."]},{"cell_type":"markdown","id":"rMOonq5OH97v","metadata":{"id":"rMOonq5OH97v"},"source":["temperature value--> how creative we want our model to be\n","\n","0 ---> temperature it means model is  very safe it is not taking any bets.\n","\n","1 --> it will take risk it might generate wrong output but it is very creative"]},{"cell_type":"markdown","id":"M9Y34zmZ8xyc","metadata":{"id":"M9Y34zmZ8xyc"},"source":["A generic interface for all LLMs. See all LLM providers: https://python.langchain.com/en/latest/modules/models/llms/integrations.html"]},{"cell_type":"markdown","id":"TB5tAUbT92Z7","metadata":{"id":"TB5tAUbT92Z7"},"source":["# **Open AI**"]},{"cell_type":"markdown","id":"BszO_ZXrs95T","metadata":{"id":"BszO_ZXrs95T"},"source":["## **Example 1**"]},{"cell_type":"code","execution_count":4,"id":"w-az-0Ex9CaD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7394,"status":"ok","timestamp":1709951929541,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"w-az-0Ex9CaD","outputId":"5392d543-2c24-436d-cf2f-009b07b865ad"},"outputs":[],"source":["#!pip install openai"]},{"cell_type":"markdown","id":"11e91701","metadata":{},"source":["#### We install openai by pip install or adding inside requirement.txt. but later will call this openai inside langchain framework\n","\n","- Here OpenAI() we are not defining LLM model name, it takes default model  **gpt-3.5-turbo**\n","- Here OPENAPI_Key also we wont mention, it reads directly from environmetal variable, but variable name should be **OPENAI_API_KEY**"]},{"cell_type":"code","execution_count":5,"id":"lJEy652utDdM","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1677,"status":"ok","timestamp":1709951934434,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"lJEy652utDdM","outputId":"1aae1a89-337f-48ab-ce0f-6fee3974c684"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n","  warn_deprecated(\n"]}],"source":["from langchain.llms import OpenAI\n","llm = OpenAI(temperature=0.9)"]},{"cell_type":"markdown","id":"n_nF4R5EtN_k","metadata":{"id":"n_nF4R5EtN_k"},"source":["And now we will pass in text and get  predictions"]},{"cell_type":"code","execution_count":10,"id":"VIUqmBl3tUgj","metadata":{"executionInfo":{"elapsed":608,"status":"ok","timestamp":1709951941761,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"VIUqmBl3tUgj"},"outputs":[],"source":["text=\"What would be a good company name for a company that makes colorful socks?\""]},{"cell_type":"markdown","id":"84979ad5","metadata":{},"source":["> Below 4 way we can use llm model in openai, best to use **invoke** method"]},{"cell_type":"code","execution_count":11,"id":"g7itCa0q9rn7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1143,"status":"ok","timestamp":1709951943644,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"g7itCa0q9rn7","outputId":"f198706e-6d15-4b9d-f8c5-d0bea51a25e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"Rainbow Feet Co.\"\n"]}],"source":["print(llm.predict(text))"]},{"cell_type":"code","execution_count":12,"id":"2KE0Fngs9daM","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1383,"status":"ok","timestamp":1709951972810,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"2KE0Fngs9daM","outputId":"3a17e0bd-cb3a-4a8f-bf81-7e478d33a409"},"outputs":[{"name":"stdout","output_type":"stream","text":[" \"Rainbow Threads\" or \"Vibrant Sox Co.\"\n"]}],"source":["print(llm(text))"]},{"cell_type":"code","execution_count":13,"id":"-s5lupvjFLVz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":574,"status":"ok","timestamp":1709952005414,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"-s5lupvjFLVz","outputId":"3cd837d1-736a-404a-a2bc-573e1a80ae59"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"Rainbow Socks Co.\" or \"Vibrant Feet Co.\" or \"Chroma Socks\" or \"Spectrum Socks\" or \"Pop of Color Socks Co.\"\n"]}],"source":["print(llm.invoke(text))"]},{"cell_type":"code","execution_count":14,"id":"c23c846e","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","SockSplash Co.\n","\n","\n","1. \"Bright Feet Co.\"\n","2. \"Socktopia\"\n","3. \"Rainbow Socks Inc.\"\n","4. \"Colorful Soles Co.\"\n","5. \"Vibrant Socks Co.\"\n","6. \"Funky Footwear Co.\"\n","7. \"Spectrum Socks\"\n","8. \"Hue Hosiery\"\n","9. \"Chroma Socks Co.\"\n","10. \"Sole Splashers\"\n","\n","\n","\"SockSplash\" or \"ChromaSocks\"\n","\n","\"Vibrant Steps Co.\"\n","\n","\n","\"Spectrum Socks\"\n","\"Rainbow Toes\"\n","\"Chroma Sox\"\n","\"Hue Hosiery\"\n","\"Vibrant Vessels\"\n","\"Dazzling Digits\"\n","\"Bright Feet Co.\"\n","\"Pigment Peds\"\n","\"Color Pop Socks\"\n","\"Kaleidosock Co.\"\n"]}],"source":["# generate --> This is to generate multiple result /5 times\n","response = llm.generate([text]*5)\n","for rest_name in response.generations:\n","    print(rest_name[0].text)"]},{"cell_type":"markdown","id":"EJIQT1FSn0Gl","metadata":{"id":"EJIQT1FSn0Gl"},"source":["## **Example 2**"]},{"cell_type":"code","execution_count":4,"id":"fa352d5f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1391,"status":"ok","timestamp":1709952621379,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"fa352d5f","outputId":"7c7a94db-56bc-4733-a1b8-2169105f5ee3","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\"Szechuan Palace\"\n"]}],"source":["from langchain.llms import OpenAI\n","\n","llm = OpenAI(temperature=0.9)\n","name = llm.predict(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n","print(name)"]},{"cell_type":"code","execution_count":5,"id":"b56e8581","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":734,"status":"ok","timestamp":1709952637758,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"b56e8581","outputId":"1657ffcb-c3fd-449a-a3f5-4b40ff35bb07"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"Imperial Jade Palace\"\n"]}],"source":["response=llm(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n","print(response)"]},{"cell_type":"code","execution_count":6,"id":"c5335b68","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\"Golden Dragon Palace\"\n"]}],"source":["response = llm.invoke(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n","print(response)"]},{"cell_type":"code","execution_count":7,"id":"ef3950b3","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"Imperial Dragon Dining\"\n","\"Imperial Wok\"\n","\n","\n","\"Dragon's Delight\" \n","\n","\n","\"The Golden Dragon Palace\"\n","\n","\"Jasmine Palace\" \n"]}],"source":["# generate --> This is to generate multiple result  /5 times\n","response = llm.generate([\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\"]*5)\n","for rest_name in response.generations:\n","    print(rest_name[0].text)"]},{"cell_type":"markdown","id":"bj6wjnKZ-bgU","metadata":{"id":"bj6wjnKZ-bgU"},"source":["# **Hugging Face**"]},{"cell_type":"markdown","id":"iTsUW116-th1","metadata":{"id":"iTsUW116-th1"},"source":["## **Example 1**\n","\n","##### We install huggingface_hub by pip install or adding inside requirement.txt. but later will call this huggingface_hub inside langchain framework\n","\n","- Here inside HuggingFaceHub(),  HUGGINGFACEHUB_API_TOKEN also we wont mention, it reads directly from environmetal variable, but variable name should be **HUGGINGFACEHUB_API_TOKEN**"]},{"cell_type":"code","execution_count":13,"id":"hDMLw7Yr-nQK","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5880,"status":"ok","timestamp":1709952974955,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"hDMLw7Yr-nQK","outputId":"60de84e7-2805-45b8-9d54-b726da6e12e2"},"outputs":[],"source":["#!pip install huggingface_hub"]},{"cell_type":"code","execution_count":14,"id":"B4w0ultA-icd","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709952978510,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"B4w0ultA-icd"},"outputs":[],"source":["from langchain import HuggingFaceHub"]},{"cell_type":"code","execution_count":15,"id":"W-pl8cXk-ie7","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"elapsed":2599,"status":"ok","timestamp":1709952987075,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"W-pl8cXk-ie7","outputId":"4d9db715-ec98-4b3f-e108-74dcc7b921a8"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.huggingface_hub.HuggingFaceHub` was deprecated in langchain-community 0.0.21 and will be removed in 0.2.0. Use HuggingFaceEndpoint instead.\n","  warn_deprecated(\n","d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["'Wie alte sind Sie?'"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# https://huggingface.co/google/flan-t5-xl\n","llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n","\n","llm(\"translate English to German: How old are you?\")"]},{"cell_type":"markdown","id":"2MOh4uIm-xDQ","metadata":{"id":"2MOh4uIm-xDQ"},"source":["## **Example 2**"]},{"cell_type":"code","execution_count":16,"id":"dmAwr5-d-z6F","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1410,"status":"ok","timestamp":1709953016522,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"dmAwr5-d-z6F","outputId":"4d769a7e-5116-4243-cbd3-4b9fd3cf4814"},"outputs":[{"name":"stdout","output_type":"stream","text":["Chinese restaurant\n"]}],"source":["from langchain import HuggingFaceHub\n","\n","llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n","name = llm.predict(\"I want to open a restaurant for Chinese food. Suggest a fency name for this.\")\n","print(name)"]},{"cell_type":"markdown","id":"0782a2dd","metadata":{"id":"0782a2dd"},"source":["# **04: Prompt Templates**"]},{"cell_type":"markdown","id":"jszTHb6J_dNV","metadata":{"id":"jszTHb6J_dNV"},"source":["- ***prompt template** used when we have **same template multiple time**\n","- Currently in the above applications we are writing an entire prompt, if you are creating a user directed application then this is not an ideal case\n","\n","- LangChain faciliates prompt management and optimization.\n","\n","- Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."]},{"cell_type":"markdown","id":"unU1DcEv7TWh","metadata":{"id":"unU1DcEv7TWh"},"source":["In many Large Language Model applications we donot pass the user input directly to the Large Language Model, we add the user input to a large piece of text called prompt template"]},{"cell_type":"markdown","id":"f463fdfa","metadata":{},"source":["## We can design PromptTemplate many way "]},{"cell_type":"markdown","id":"IWqka6F_93QB","metadata":{"id":"IWqka6F_93QB"},"source":["### **Way 1:** Example 1"]},{"cell_type":"code","execution_count":17,"id":"7a306b9d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":583,"status":"ok","timestamp":1709953230582,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"7a306b9d","outputId":"c5a1a37d-aade-4bed-96fd-cebf91e6d46c","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["I want to open a restaurant for Italian food. Suggest a fency name for this.\n"]}],"source":["from langchain.prompts import PromptTemplate\n","llm = OpenAI(temperature=0.9)\n","\n","prompt = PromptTemplate(\n","    input_variables =['cuisine'],\n","    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",")\n","p = prompt.format(cuisine=\"Italian\")\n","print(p)"]},{"cell_type":"markdown","id":"qlKeWd7B95-R","metadata":{"id":"qlKeWd7B95-R"},"source":["### **Way 2:** Example 2"]},{"cell_type":"code","execution_count":18,"id":"qqJZBS9u8534","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":562,"status":"ok","timestamp":1709953255360,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"qqJZBS9u8534","outputId":"7e38af43-1ec3-49c0-fe5b-1ea18ec86492"},"outputs":[{"name":"stdout","output_type":"stream","text":["What is a good name for a company that makes colorful socks\n"]}],"source":["from langchain.prompts import PromptTemplate\n","prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n","p = prompt.format(product=\"colorful socks\")\n","print(p)"]},{"cell_type":"markdown","id":"af406b92","metadata":{"id":"af406b92"},"source":["# **05: Chains**\n","\n","- Combine LLMs and Prompts in multi-step workflows\n","## **Without Chain running one after the other**"]},{"cell_type":"markdown","id":"lcjlXP7z_-k6","metadata":{"id":"lcjlXP7z_-k6"},"source":["Now as we have the  **model**:\n","\n","\n","  llm = OpenAI(temperature=0.9)\n","\n","\n","and the **Prompt Template**:\n","\n","prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n","\n","\n","prompt.format(product=\"colorful socks\")\n","\n","\n","Now using Chains we will link together model and the PromptTemplate and other Chains"]},{"cell_type":"markdown","id":"mIJx5zL2BbHJ","metadata":{"id":"mIJx5zL2BbHJ"},"source":["The simplest and most common type of Chain is LLMChain, which passes the input first to Prompt Template and then to Large Language Model"]},{"cell_type":"markdown","id":"5icZHtlDFrpI","metadata":{"id":"5icZHtlDFrpI"},"source":["LLMChain is responsible to execute the PromptTemplate, For every PromptTemplate we will specifically have an LLMChain"]},{"cell_type":"markdown","id":"MAUSugfLCZH-","metadata":{"id":"MAUSugfLCZH-"},"source":["### **Example 1**"]},{"cell_type":"code","execution_count":19,"id":"22NEqcvGGvHJ","metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1709953338325,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"22NEqcvGGvHJ"},"outputs":[],"source":["#Invoke LLM Model \n","from langchain.llms import OpenAI\n","\n","llm = OpenAI(temperature=0.9)"]},{"cell_type":"code","execution_count":20,"id":"bK-KESsGGOhY","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":573,"status":"ok","timestamp":1709953365393,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"bK-KESsGGOhY","outputId":"0ea058be-573d-411b-998c-38b54eb26ab9"},"outputs":[{"data":{"text/plain":["'What is a good name for a company that makes colorful socks'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["#Create PromptTemplate\n","from langchain.prompts import PromptTemplate\n","prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n","prompt.format(product=\"colorful socks\")"]},{"cell_type":"markdown","id":"8KjGw4iXGUGJ","metadata":{"id":"8KjGw4iXGUGJ"},"source":["Whatever input text i am giving that will get assigned to this particular variable that is **product**"]},{"cell_type":"code","execution_count":21,"id":"1gatUl_ICZOP","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2079,"status":"ok","timestamp":1709953384153,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"1gatUl_ICZOP","outputId":"9b395f76-b1f4-4d84-e2d9-56d9c94bec39"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\"Rainbow Socks Co.\" \n"]}],"source":["#Pass PromptTemplate to LLM via LLMChain\n","from langchain.chains import LLMChain\n","\n","#Define LLMChain with model name and prompt\n","chain = LLMChain(llm=llm, prompt=prompt)\n","\n","#Execute chain by passing variable \"product\"'s value\n","response= chain.run(\"colorful socks\")\n","print(response)"]},{"cell_type":"markdown","id":"O93s1iRICXNv","metadata":{"id":"O93s1iRICXNv"},"source":["### **Example 2**"]},{"cell_type":"code","execution_count":22,"id":"qV_H_EGCG-OR","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709953459678,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"qV_H_EGCG-OR"},"outputs":[],"source":["#Invoke LLM Model \n","from langchain.llms import OpenAI\n","\n","llm = OpenAI(temperature=0.9)"]},{"cell_type":"code","execution_count":23,"id":"uLtIkYe6G7xK","metadata":{"executionInfo":{"elapsed":561,"status":"ok","timestamp":1709953468421,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"uLtIkYe6G7xK"},"outputs":[],"source":["#Create PromptTemplate\n","from langchain.prompts import PromptTemplate\n","\n","prompt_template_name = PromptTemplate(\n","    input_variables =['cuisine'],\n","    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",")"]},{"cell_type":"code","execution_count":24,"id":"ba65c213","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1392,"status":"ok","timestamp":1709953478020,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"ba65c213","outputId":"fc666bbb-ec12-4904-90d4-9f7fe9eb355a","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"Casa de Sabores\" (House of Flavors)\n"]}],"source":["#Pass PromptTemplate to LLM via LLMChain\n","from langchain.chains import LLMChain\n","\n","#Define LLMChain with model name and prompt\n","chain = LLMChain(llm=llm, prompt=prompt_template_name)\n","\n","#Execute chain by passing variable \"product\"'s value\n","response=chain.run(\"Mexican\")\n","print(response)"]},{"cell_type":"code","execution_count":25,"id":"e5ccee75","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1372,"status":"ok","timestamp":1709953518179,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"e5ccee75","outputId":"c9b29fde-bd70-4345-d88e-6001a40a2dad","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new LLMChain chain...\u001b[0m\n","Prompt after formatting:\n","\u001b[32;1m\u001b[1;3mI want to open a restaurant for Mexican food. Suggest a fency name for this.\u001b[0m\n"]},{"name":"stdout","output_type":"stream","text":["\n","\u001b[1m> Finished chain.\u001b[0m\n","\n","\n","\"Casa de Sabores\" (\"House of Flavors\")\n"]}],"source":["#Here verbose = True tells which step/statement executing etc. log will be displayed\n","chain = LLMChain(llm=llm, prompt=prompt_template_name, verbose=True)\n","response=chain.run(\"Mexican\")\n","print(response)"]},{"cell_type":"markdown","id":"87a98d9f","metadata":{"id":"87a98d9f"},"source":["## 5.1. **Simple Sequential Chain**\n","\n","- Can we combine Multiple PromptTemplates, We will try to **combine Multiple PromptTemplates**\n","- The output from the **first PromptTemplate is passed to the next PromptTemplate** as input\n","- To comine the Chain and  to set a sequence for that we use **SimpleSequentialChain**"]},{"cell_type":"code","execution_count":26,"id":"21098937","metadata":{"executionInfo":{"elapsed":549,"status":"ok","timestamp":1709953688653,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"21098937"},"outputs":[],"source":["llm = OpenAI(temperature=0.6)\n","\n","#Individual template 1\n","prompt_template_name = PromptTemplate(\n","    input_variables =['cuisine'],\n","    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",")\n","\n","#Response from Individual template 1\n","name_chain =LLMChain(llm=llm, prompt=prompt_template_name)\n","\n","#Individual template 2\n","prompt_template_items = PromptTemplate(\n","    input_variables = ['restaurant_name'],\n","    template=\"\"\"Suggest some menu items for {restaurant_name}\"\"\"\n",")\n","\n","#Response from Individual template 2\n","food_items_chain = LLMChain(llm=llm, prompt=prompt_template_items)"]},{"cell_type":"code","execution_count":27,"id":"d9fd9a79","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2790,"status":"ok","timestamp":1709953693730,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"d9fd9a79","outputId":"4928fbd5-8008-4aed-f698-c60d8f77d411","scrolled":false},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","1. Masala Dosa - a popular South Indian dish made with crispy crepe-like dosa filled with a spicy potato filling.\n","2. Uttapam - a thick pancake-like dish made with a batter of rice and lentils, topped with vegetables and served with chutney and sambar.\n","3. Medu Vada - savory deep-fried doughnuts made with lentil batter, served with chutney and sambar.\n","4. Vegetable Biryani - fragrant basmati rice cooked with mixed vegetables, spices, and herbs.\n","5. Sambar - a flavorful lentil-based stew with vegetables, served as a side dish.\n","6. Rasam - a tangy and spicy tomato-based soup, served as an appetizer.\n","7. Ghee Roast - a dish made with marinated chicken or paneer cooked in a spicy ghee-based sauce.\n","8. Puri Bhaji - deep-fried puffed bread served with a spicy potato curry.\n","9. Rava Idli - soft and fluffy steamed semolina cakes, served with chutney and sambar.\n","10. Payasam - a sweet and creamy pudding made with vermicelli, milk, and sugar, served as a dessert.\n"]}],"source":["from langchain.chains import SimpleSequentialChain\n","\n","#Combine Individual template 1 and Individual template 2 using SimpleSequentialChain\n","#Response from Individual template 1 will passed to input variable to Individual template 2\n","chain = SimpleSequentialChain(chains = [name_chain, food_items_chain])\n","\n","#Execute chain by passing variable \"cuisine\"'s value\n","content = chain.run(\"udupi\")  # Here we pass cuisine = 'indian' to SimpleSequentialChain \n","print(content)"]},{"cell_type":"markdown","id":"njqmmiouJ6Uc","metadata":{"id":"njqmmiouJ6Uc"},"source":["### Issue in **SimpleSequentialChain**\n","- There is a issue with **SimpleSequentialChain** it only shows last **input information**\n","- To show the entire information i will use **SequentialChain**"]},{"cell_type":"markdown","id":"0386d05c","metadata":{"id":"0386d05c"},"source":["## 5.2. **Sequential Chain**\n","\n","- Its same as **simplesequentialchain** just difference is here, we define **output_key**, so simple seq gives only last o/p , this gives both\n","- If we want all the prompt output then use  **Sequential Chain**"]},{"cell_type":"code","execution_count":28,"id":"49dc0fae","metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1709953716635,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"49dc0fae"},"outputs":[],"source":["llm = OpenAI(temperature=0.7)\n","\n","prompt_template_name = PromptTemplate(\n","    input_variables =['cuisine'],\n","    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",")\n","\n","name_chain =LLMChain(llm=llm, prompt=prompt_template_name, output_key=\"restaurant_name\")"]},{"cell_type":"code","execution_count":29,"id":"9dea8402","metadata":{"executionInfo":{"elapsed":567,"status":"ok","timestamp":1709953729385,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"9dea8402"},"outputs":[],"source":["llm = OpenAI(temperature=0.7)\n","\n","prompt_template_items = PromptTemplate(\n","    input_variables = ['restaurant_name'],\n","    template=\"Suggest some menu items for {restaurant_name}.\"\n",")\n","\n","food_items_chain =LLMChain(llm=llm, prompt=prompt_template_items, output_key=\"menu_items\")"]},{"cell_type":"code","execution_count":30,"id":"1ec1be10","metadata":{"executionInfo":{"elapsed":593,"status":"ok","timestamp":1709953741513,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"1ec1be10"},"outputs":[],"source":["from langchain.chains import SequentialChain\n","\n","chain = SequentialChain(\n","    chains = [name_chain, food_items_chain],\n","    input_variables = ['cuisine'],\n","    output_variables = ['restaurant_name', \"menu_items\"]\n",")"]},{"cell_type":"code","execution_count":31,"id":"4653c540","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3239,"status":"ok","timestamp":1709953772335,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"4653c540","outputId":"6d92a3b3-7f64-4908-db5d-18a0175c4e6d"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":["{'cuisine': 'indian', 'restaurant_name': '\\n\\n\"Taj Mahal Spice\" ', 'menu_items': '\\n1. Tandoori Chicken: Marinated chicken roasted in a clay oven and served with a side of mint chutney. \\n2. Butter Chicken: A creamy and flavorful chicken curry made with a blend of spices and served with basmati rice. \\n3. Vegetable Samosas: Crispy fried pastries filled with spiced potatoes, peas, and onions. \\n4. Lamb Biryani: Fragrant basmati rice cooked with tender lamb pieces, spices, and herbs. \\n5. Saag Paneer: A popular vegetarian dish made with spinach and chunks of paneer cheese in a creamy tomato-based sauce. \\n6. Garlic Naan: Freshly baked flatbread brushed with garlic butter. \\n7. Masala Dosa: A crispy crepe stuffed with a savory potato and vegetable filling, served with chutneys and sambar. \\n8. Chicken Tikka Masala: A classic dish of chicken in a rich and spicy tomato-based sauce. \\n9. Aloo Gobi: A vegetarian dish made with cauliflower and potatoes, cooked with aromatic Indian spices. \\n10. Mango Lassi: A refreshing yogurt-based drink made with fresh mango puree and a touch of cardamom.'}\n"]}],"source":["#Execute chain by passing variable \"cuisine\"'s value. \n","# here chain.run() not used to execute. It calls by below method directly\n","print(chain({\"cuisine\": \"indian\"}))"]},{"cell_type":"markdown","id":"4069a75e","metadata":{"id":"4069a75e"},"source":["# **06. Agents and Tools**\n","\n","- Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n","\n","- When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n","\n","    - **Tool:** A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n","    - **LLM:** The language model powering the agent.\n","    - **Agent:** The agent to use.\n"]},{"cell_type":"markdown","id":"GgNLQ6kSL4na","metadata":{"id":"GgNLQ6kSL4na"},"source":["## Agent is a very powerful concept in LangChain\n","\n","- For example I have to travel from Dubai to Canada, I type this in ChatGPT\n","\n","\n","\n","---> Give me  two flight options from Dubai to Canada on September 1, 2023 | ChatGPT will not be able to answer because has knowledge till\n","September 2021\n","\n","\n","\n","ChatGPT plus has Expedia Plugin, if we enable this plugin it will go to Expedia Plugin and will try to pull information about Flights & it will show the information"]},{"cell_type":"markdown","id":"_nC7hejzNcXC","metadata":{"id":"_nC7hejzNcXC"},"source":["#**What exactly happens when we try to enable this plugin**"]},{"cell_type":"markdown","id":"DxDAjwe5SMEe","metadata":{"id":"DxDAjwe5SMEe"},"source":["![image.png](data:image/png;base64,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)\n","\n","\n","- When we think about LLM. Many people think that it is just a knowledge engine, it has knowledge and it will try to give answer based on that knowledge but the knowledge is only limited to September 2021. The think that most people missout is that Large Lanaguage Model has a reasoning engine, and using that reasoning engine it can figure out when someone types this type of Question\n","\n","- Give me  two flight options from Dubai to Canada on September 1, 2023\n","- As a human we go to Expedia as we have a reasoning engine in our brain.\n","\n","- LLM has a reasoning engine as well, so it will figure out the Source, Destination, Date and it will call Expedia Plugin and it will return response back."]},{"cell_type":"markdown","id":"LJFY2fPgS-hH","metadata":{"id":"LJFY2fPgS-hH"},"source":["![image.png](data:image/png;base64,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)"]},{"cell_type":"markdown","id":"xfPzN1KrTAE6","metadata":{"id":"xfPzN1KrTAE6"},"source":["**How much is US GDP in 2022? plus 5**\n","\n","- As LLM has a reasoning engine to answer that question it will go to Google Search Tool, it will find that answer and then it will use Math Tool and do plus 5\n","\n","**An agent has access to a suite of tools, and determines which ones to use depending on the user input.**\n","\n","#### Agent will conenct with external tools and it will use LLM reasoning capabilities\n","\n","- All the tools like **Google Search Tool and Math Tool are available as part of LangChain and you can configure  agent, so agent is nothing but using all these tools and LLM reasoning capabilities** to perform a given task "]},{"cell_type":"markdown","id":"V-v5l0EL5Om7","metadata":{"id":"V-v5l0EL5Om7"},"source":["### To access Google Search Results in Real Time we use **serpapi**"]},{"cell_type":"markdown","id":"a06e9ffe","metadata":{},"source":["## List all the different Tools in Agents"]},{"cell_type":"code","execution_count":16,"id":"e19199a0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[   'requests',\n","    'requests_get',\n","    'requests_post',\n","    'requests_patch',\n","    'requests_put',\n","    'requests_delete',\n","    'terminal',\n","    'sleep',\n","    'wolfram-alpha',\n","    'google-search',\n","    'google-search-results-json',\n","    'searx-search-results-json',\n","    'bing-search',\n","    'metaphor-search',\n","    'ddg-search',\n","    'google-lens',\n","    'google-serper',\n","    'google-scholar',\n","    'google-finance',\n","    'google-trends',\n","    'google-jobs',\n","    'google-serper-results-json',\n","    'searchapi',\n","    'searchapi-results-json',\n","    'serpapi',\n","    'dalle-image-generator',\n","    'twilio',\n","    'searx-search',\n","    'merriam-webster',\n","    'wikipedia',\n","    'arxiv',\n","    'golden-query',\n","    'pubmed',\n","    'human',\n","    'awslambda',\n","    'stackexchange',\n","    'sceneXplain',\n","    'graphql',\n","    'openweathermap-api',\n","    'dataforseo-api-search',\n","    'dataforseo-api-search-json',\n","    'eleven_labs_text2speech',\n","    'google_cloud_texttospeech',\n","    'reddit_search',\n","    'news-api',\n","    'tmdb-api',\n","    'podcast-api',\n","    'memorize',\n","    'llm-math',\n","    'open-meteo-api']\n"]}],"source":["from langchain.agents import get_all_tool_names\n","import pprint\n","\n","pp = pprint.PrettyPrinter(indent=4)\n","pp.pprint(get_all_tool_names())"]},{"cell_type":"markdown","id":"471b2c6b","metadata":{"id":"471b2c6b"},"source":["## 6.1 **AGENT: serpapi(Google serach) and llm-math tool**\n","\n","- **SerpApi** is a **real-time API** to access Google search results.\n","https://serpapi.com/\n","\n","- If you're using a **text LLM**, first try **zero-shot-react-description**, aka. the **MRKL agent for LLMs**.\n","\n","\n","- If you're using a **Chat Model**, try **chat-zero-shot-react-descriptio**n, aka. the **MRKL agent for Chat** Models.\n","\n","\n","- If you're using a **Chat Model and want to use memory**, try **chat-conversational-react-description**, the **Conversational agent**.\n","\n","\n","- If you have a **complex task** that requires many steps and you're interested in experimenting with a new type of agent, try the **Plan-and-Execute agent**."]},{"cell_type":"code","execution_count":32,"id":"5Mjy8I20lmdD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7645,"status":"ok","timestamp":1709954025164,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"5Mjy8I20lmdD","outputId":"0573d82c-b1ad-4fb2-9a18-4b686e7af60f"},"outputs":[],"source":["#!pip install google-search-results\n","#!pip install numexpr"]},{"cell_type":"markdown","id":"745b7b47","metadata":{},"source":["- **Wikipedia is open source api**, so while reading this we doesn't mention any API KEY,  BUT **Serpapi** key used for  **Google search**  just **load_tool name will change from serpapi or wikipedia**"]},{"cell_type":"code","execution_count":33,"id":"QOK4FGizmihA","metadata":{"executionInfo":{"elapsed":698,"status":"ok","timestamp":1709954185103,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"QOK4FGizmihA"},"outputs":[{"name":"stdout","output_type":"stream","text":["790f7\n"]}],"source":["import os\n","\n","#os.environ['SERPAPI_API_KEY'] = 'Give SERPAPI_API_KEY'\n","SERPAPI_API_KEY=os.getenv(\"SERPAPI_API_KEY\")  \n","print(SERPAPI_API_KEY[:5])"]},{"cell_type":"code","execution_count":19,"id":"fec4212d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"executionInfo":{"elapsed":49739,"status":"ok","timestamp":1709954681177,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"fec4212d","outputId":"31327138-4f6a-4345-e7bf-815f0f723c54","scrolled":false},"outputs":[{"ename":"ValidationError","evalue":"1 validation error for SerpAPIWrapper\n__root__\n  Did not find serpapi_api_key, please add an environment variable `SERPAPI_API_KEY` which contains it, or pass `serpapi_api_key` as a named parameter. (type=value_error)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[1;32mIn[19], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Here using Google Search API - serpapi\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# The tools we'll give the Agent access to is llm-math\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Note that the 'llm-math' tool uses an LLM, so we need to pass that in. Which is fixed\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m tools \u001b[38;5;241m=\u001b[39m \u001b[43mload_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mserpapi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm-math\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m agent \u001b[38;5;241m=\u001b[39m initialize_agent(tools, llm, agent\u001b[38;5;241m=\u001b[39mAgentType\u001b[38;5;241m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","File \u001b[1;32md:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain\\agents\\load_tools.py:614\u001b[0m, in \u001b[0;36mload_tools\u001b[1;34m(tool_names, llm, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m     _get_tool_func, extra_keys \u001b[38;5;241m=\u001b[39m _EXTRA_OPTIONAL_TOOLS[name]\n\u001b[0;32m    613\u001b[0m     sub_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m extra_keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs}\n\u001b[1;32m--> 614\u001b[0m     tool \u001b[38;5;241m=\u001b[39m _get_tool_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msub_kwargs)\n\u001b[0;32m    615\u001b[0m     tools\u001b[38;5;241m.\u001b[39mappend(tool)\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[1;32md:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain\\agents\\load_tools.py:301\u001b[0m, in \u001b[0;36m_get_serpapi\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_serpapi\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTool:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tool(\n\u001b[0;32m    299\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    300\u001b[0m         description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA search engine. Useful for when you need to answer questions about current events. Input should be a search query.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 301\u001b[0m         func\u001b[38;5;241m=\u001b[39mSerpAPIWrapper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mrun,\n\u001b[0;32m    302\u001b[0m         coroutine\u001b[38;5;241m=\u001b[39mSerpAPIWrapper(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39marun,\n\u001b[0;32m    303\u001b[0m     )\n","File \u001b[1;32md:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n","\u001b[1;31mValidationError\u001b[0m: 1 validation error for SerpAPIWrapper\n__root__\n  Did not find serpapi_api_key, please add an environment variable `SERPAPI_API_KEY` which contains it, or pass `serpapi_api_key` as a named parameter. (type=value_error)"]}],"source":["from langchain.agents import AgentType, initialize_agent, load_tools\n","from langchain.llms import OpenAI\n","\n","llm = OpenAI(temperature=0)\n","\n","# Here using Google Search API - serpapi\n","# The tools we'll give the Agent access to is llm-math\n","# Note that the 'llm-math' tool uses an LLM, so we need to pass that in. Which is fixed\n","tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n","\n","# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n","agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n","\n","# Let's test it out!\n","agent.run(\"What was the GDP of US in 2023?\")"]},{"cell_type":"markdown","id":"09cd3a12","metadata":{"id":"09cd3a12"},"source":["## 6.2 **AGENT: Wikipedia and llm-math tool**\n","\n","- **Wikipedia is open source api**, so while reading this we doesn't mention any API KEY,  BUT **Serpapi** key used for  **Google search**  just **load_tool name will change from serpapi or wikipedia**"]},{"cell_type":"code","execution_count":20,"id":"14d06ce6","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"elapsed":46719,"status":"ok","timestamp":1709954754388,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"14d06ce6","outputId":"51a3e802-3a03-4d0a-bdce-c41796af9d79","scrolled":false},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n","  warn_deprecated(\n","d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n","  warn_deprecated(\n"]},{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the release year of the film and then use the Calculator to raise it to the 0.43 power.\n","Action: wikipedia\n","Action Input: Departed film\u001b[0m"]},{"name":"stderr","output_type":"stream","text":["d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\wikipedia\\wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"html.parser\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n","\n","The code that caused this warning is on line 389 of the file d:\\Prabha\\Data Science\\Prabha-DS\\Gen_AI\\Ineuron\\Gen_AI_Course\\Project\\venv\\lib\\site-packages\\wikipedia\\wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"html.parser\"' to the BeautifulSoup constructor.\n","\n","  lis = BeautifulSoup(html).find_all('li')\n"]},{"name":"stdout","output_type":"stream","text":["\n","Observation: \u001b[36;1m\u001b[1;3mPage: The Departed\n","Summary: The Departed is a 2006 American crime thriller film directed by Martin Scorsese and written by William Monahan. It is both a remake of the 2002 Hong Kong film Infernal Affairs and also loosely based on the real-life Boston Winter Hill Gang; the character Colin Sullivan is based on the corrupt FBI agent John Connolly, while the character Frank Costello is based on Irish-American gangster and crime boss Whitey Bulger. The film stars Leonardo DiCaprio, Matt Damon, Jack Nicholson, and Mark Wahlberg, with Martin Sheen, Ray Winstone, Vera Farmiga, Alec Baldwin, Anthony Anderson and James Badge Dale in supporting roles.\n","The film takes place in Boston and the surrounding metro area, primarily in the South Boston neighborhood. Irish Mob boss Frank Costello (Nicholson) plants Colin Sullivan (Damon) as a spy within the Massachusetts State Police; simultaneously, the police assign undercover state trooper Billy Costigan (DiCaprio) to infiltrate Costello's mob crew. When both sides realize the situation, Sullivan and Costigan each attempt to discover the other's identity before they are found out.\n","The Departed was a critical and commercial success, receiving acclaim for its direction, performances (particularly of DiCaprio, Nicholson, and Wahlberg), screenplay, and editing.\n","It won several accolades, including four Oscars at the 79th Academy Awards: for Best Picture, Best Director, Best Adapted Screenplay, and Best Film Editing. It remains Scorsese's only personal Oscar win. The film also received six nominations each at the 64th Golden Globe Awards and the 60th British Academy Film Awards, and two nominations at the 13th Screen Actors Guild Awards.\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m Now I can use the Calculator to raise the release year to the 0.43 power.\n","Action: Calculator\n","Action Input: 2006 ^ 0.43\u001b[0m\n","Observation: \u001b[33;1m\u001b[1;3mAnswer: 26.30281917656938\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n","Final Answer: 26.30281917656938\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"text/plain":["'26.30281917656938'"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# install this package: pip install wikipedia\n","llm = OpenAI(temperature=0)\n","\n","# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n","tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n","\n","# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n","agent = initialize_agent(\n","    tools,\n","    llm,\n","    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","    verbose=True\n",")\n","\n","# Let's test it out!\n","agent.run(\"In what year was the film Departed with Leopnardo Dicaprio released? What is this year raised to the 0.43 power?\")"]},{"cell_type":"markdown","id":"a0735921","metadata":{},"source":["## 6.3 **AGENT: Human as a tool**\n","\n","- Here it prompts **Human**, for ans"]},{"cell_type":"code","execution_count":23,"id":"28793475","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n","\u001b[32;1m\u001b[1;3m I should ask a human for guidance\n","Action: human\n","Action Input: Who born in Uppinakudru\u001b[0m\n","\n","Who born in Uppinakudru\n","\n","Observation: \u001b[36;1m\u001b[1;3mPrabha\u001b[0m\n","Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n","Final Answer: Prabha\u001b[0m\n","\n","\u001b[1m> Finished chain.\u001b[0m\n"]},{"data":{"text/plain":["'Prabha'"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# install this package: pip install wikipedia\n","llm = OpenAI(temperature=0)\n","\n","# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n","tools = load_tools([\"human\", \"llm-math\"],   llm)\n","\n","\n","# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n","agent = initialize_agent(\n","    tools,\n","    llm,\n","    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n","    verbose=True\n",")\n","\n","# Let's test it out!\n","agent.run(\"Who born in Uppinakudru\")"]},{"cell_type":"markdown","id":"b6be7ee7","metadata":{"id":"b6be7ee7"},"source":["# **07: Memory**\n","\n","- Chatbot application like ChatGPT, you will notice that it remember past information\n","- **Conversation chain** and **Conversation buffer memory** are same only, It keep on adds content/ q&a respnse to memory, keeps growing"]},{"cell_type":"code","execution_count":36,"id":"Iqunzha0Ztuz","metadata":{"executionInfo":{"elapsed":620,"status":"ok","timestamp":1709954791387,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"Iqunzha0Ztuz"},"outputs":[],"source":["from langchain.llms import OpenAI\n","\n","llm = OpenAI(temperature=0.9)"]},{"cell_type":"code","execution_count":37,"id":"NE-poGM1Zxss","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709954834131,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"NE-poGM1Zxss"},"outputs":[],"source":["from langchain.prompts import PromptTemplate\n","\n","prompt_template_name = PromptTemplate(\n","    input_variables =['cuisine'],\n","    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",")"]},{"cell_type":"code","execution_count":38,"id":"2acab5d0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":587,"status":"ok","timestamp":1709954837244,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"2acab5d0","outputId":"20c4dbec-6c18-4523-9439-bbfc74e7adce"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\"Casa de Sabor\" \n"]}],"source":["#Create chain\n","from langchain.chains import LLMChain\n","\n","chain = LLMChain(llm=llm,prompt=prompt_template_name)\n","\n","#1st execution\n","name = chain.run(\"Mexican\")\n","print(name)"]},{"cell_type":"code","execution_count":39,"id":"5bc200f9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":785,"status":"ok","timestamp":1709954841237,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"5bc200f9","outputId":"986959c9-ecfb-414e-c211-42b5b23f84fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"Masala Mansion\" \n"]}],"source":["#2nd execution\n","name = chain.run(\"Indian\")\n","print(name)"]},{"cell_type":"code","execution_count":40,"id":"229a6888","metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1709954842911,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"229a6888"},"outputs":[],"source":["chain.memory"]},{"cell_type":"code","execution_count":41,"id":"f492fb5a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":602,"status":"ok","timestamp":1709954846619,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"f492fb5a","outputId":"d9f95160-56dd-41b9-f049-d07c5eec61f7","scrolled":true},"outputs":[{"data":{"text/plain":["NoneType"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["type(chain.memory)"]},{"cell_type":"markdown","id":"b0c1cff6","metadata":{},"source":["> If we notice above, it not able to remember previous query or response"]},{"cell_type":"markdown","id":"871492be","metadata":{"id":"871492be"},"source":["## 7.1 **ConversationBufferMemory**\n","We can attach memory to remember all previous conversation"]},{"cell_type":"code","execution_count":42,"id":"53eea298","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":698,"status":"ok","timestamp":1709954857716,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"53eea298","outputId":"945fa19d-7700-4cb2-cd89-fbe7ba7005aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"La Fiesta de Sabores\" \n"]}],"source":["from langchain.memory import ConversationBufferMemory\n","\n","# Adding ConversationBufferMemory\n","memory = ConversationBufferMemory()\n","\n","chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory) # Adding memory in chain\n","#1st execution\n","name = chain.run(\"Mexican\")\n","print(name)"]},{"cell_type":"code","execution_count":43,"id":"0de5d50b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1121,"status":"ok","timestamp":1709954863195,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"0de5d50b","outputId":"3a6bdb74-2992-49d0-a51b-c505b0b63c7a"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\"Al-Farah Palace\"\n"]}],"source":["#2nd execution\n","name = chain.run(\"Arabic\")\n","print(name)"]},{"cell_type":"code","execution_count":44,"id":"5cc88888","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1709954864966,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"5cc88888","outputId":"37935413-ae8b-4008-b1ae-ac6a2becc221","scrolled":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Human: Mexican\n","AI: \n","\n","\"La Fiesta de Sabores\" \n","Human: Arabic\n","AI: \n","\n","\"Al-Farah Palace\"\n"]}],"source":["#Check memory\n","print(chain.memory.buffer)"]},{"cell_type":"markdown","id":"f95dd6e0","metadata":{},"source":["\n","- Conversation buffer memory goes growing endlessly, It takes huge space"]},{"cell_type":"markdown","id":"a0a88b5b","metadata":{"id":"a0a88b5b"},"source":["## 7.2 **ConversationChain**\n","\n","- This is same like **Conversation buffer memory**, It keep on adds content/ q&a respnse to memory, keeps growing\n","- It behaves like AI and human conversation"]},{"cell_type":"code","execution_count":45,"id":"687ddd2f","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":555,"status":"ok","timestamp":1709954994063,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"687ddd2f","outputId":"21dfaae1-c157-495c-d48c-c5880e494368"},"outputs":[{"name":"stdout","output_type":"stream","text":["The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n","\n","Current conversation:\n","{history}\n","Human: {input}\n","AI:\n"]}],"source":["from langchain.chains import ConversationChain\n","\n","#Define Conversationchain, It behaves like AI and human conversation\n","convo = ConversationChain(llm=OpenAI(temperature=0.7))\n","print(convo.prompt.template)"]},{"cell_type":"code","execution_count":46,"id":"47ad5062","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":1105,"status":"ok","timestamp":1709955120967,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"47ad5062","outputId":"196c7dd4-aeff-41a5-d94d-5877f324003d"},"outputs":[{"data":{"text/plain":["\" The first cricket world cup was held in 1975 and was won by the West Indies team. The final match was played at Lord's Cricket Ground in London, England and the West Indies defeated Australia by 17 runs. It was a historic moment for both teams as it was the first ever cricket world cup and the West Indies became the first ever champions.\""]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"Who won the first cricket world cup?\")"]},{"cell_type":"code","execution_count":47,"id":"03c80b54","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":1148,"status":"ok","timestamp":1709955127108,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"03c80b54","outputId":"3ba9b4bf-1c06-430a-e90a-6537f56cfdb6"},"outputs":[{"data":{"text/plain":["' The answer to 5+5 is 10.'"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"How much is 5+5?\")"]},{"cell_type":"code","execution_count":48,"id":"07342f88","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"executionInfo":{"elapsed":1386,"status":"ok","timestamp":1709955131491,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"07342f88","outputId":"2db815b3-48e9-4a41-d923-5ac1fbd83af1"},"outputs":[{"data":{"text/plain":["\" The captain of the winning West Indies team was Clive Lloyd. He was a legendary cricketer who led his team to victory in the first two cricket world cups in 1975 and 1979. He was known for his powerful batting and strategic captaincy, making him a key player in the team's success.\""]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"Who was the captain ofthe winning team?\")"]},{"cell_type":"code","execution_count":49,"id":"4e459d07","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":586,"status":"ok","timestamp":1709955134239,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"4e459d07","outputId":"728be2a1-32c6-4671-c319-5b4f66c735bc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Human: Who won the first cricket world cup?\n","AI:  The first cricket world cup was held in 1975 and was won by the West Indies team. The final match was played at Lord's Cricket Ground in London, England and the West Indies defeated Australia by 17 runs. It was a historic moment for both teams as it was the first ever cricket world cup and the West Indies became the first ever champions.\n","Human: How much is 5+5?\n","AI:  The answer to 5+5 is 10.\n","Human: Who was the captain ofthe winning team?\n","AI:  The captain of the winning West Indies team was Clive Lloyd. He was a legendary cricketer who led his team to victory in the first two cricket world cups in 1975 and 1979. He was known for his powerful batting and strategic captaincy, making him a key player in the team's success.\n"]}],"source":["print(convo.memory.buffer)"]},{"cell_type":"markdown","id":"feaa3abd","metadata":{"id":"feaa3abd"},"source":["## 7.3 **ConversationBufferWindowMemory**\n","\n","- Conversation buffer memory and ConversationChain goes growing endlessly, It takes huge space\n","\n","- It  Just remember last 5 Conversation Chain based on we set parameter k=5\n","\n","- It Just remember last 10-20 Conversation Chain based on we set parameter k=5\n"," "]},{"cell_type":"code","execution_count":50,"id":"460eb33c","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":107},"executionInfo":{"elapsed":3582,"status":"ok","timestamp":1709955226395,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"460eb33c","outputId":"309fb635-8710-4f4b-dca5-e2699290fd7e"},"outputs":[{"data":{"text/plain":["' The first cricket world cup was won by the West Indies in 1975.'"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["from langchain.memory import ConversationBufferWindowMemory\n","\n","memory = ConversationBufferWindowMemory(k=1)\n","\n","convo = ConversationChain(\n","    llm=OpenAI(temperature=0.7),\n","    memory=memory\n",")\n","convo.run(\"Who won the first cricket world cup?\")"]},{"cell_type":"code","execution_count":51,"id":"d395beaf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":1147,"status":"ok","timestamp":1709955227540,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"d395beaf","outputId":"2af1f76e-f461-4f01-9dce-b3f06908eccd"},"outputs":[{"data":{"text/plain":["'  5+5 is equal to 10.'"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"How much is 5+5?\")"]},{"cell_type":"code","execution_count":52,"id":"93b24745","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":808,"status":"ok","timestamp":1709955231732,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"93b24745","outputId":"72e2d750-8b63-44be-d761-a29bf9f9ab5a"},"outputs":[{"data":{"text/plain":["' I am not sure which specific team you are referring to. Do you have any more context or information?'"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"Who was the captain of the winning team?\")"]},{"cell_type":"code","execution_count":53,"id":"K63Ie5FTvjzo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":628,"status":"ok","timestamp":1709955236197,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"K63Ie5FTvjzo","outputId":"8384658a-6b2e-4dc4-c18d-174ad954e0a6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Human: Who was the captain of the winning team?\n","AI:  I am not sure which specific team you are referring to. Do you have any more context or information?\n"]}],"source":["print(convo.memory.buffer)"]},{"cell_type":"code","execution_count":54,"id":"e8fd6c6c","metadata":{},"outputs":[{"data":{"text/plain":["' The first cricket world cup was won by the West Indies team in 1975. It was held in England and the final match was played between the West Indies and Australia. The West Indies won by 17 runs.'"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["#ConversationBufferWindowMemory(k=3)  --> Remember last 3 conversation\n","memory = ConversationBufferWindowMemory(k=3)\n","\n","convo = ConversationChain(\n","    llm=OpenAI(temperature=0.7),\n","    memory=memory\n",")\n","convo.run(\"Who won the first cricket world cup?\")"]},{"cell_type":"code","execution_count":55,"id":"c04e3163","metadata":{},"outputs":[{"data":{"text/plain":["'  The answer to 10+10 is 20. This is because 10 plus 10 equals 20. It is a simple arithmetic equation that can be easily solved. Is there anything else you would like to know?'"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"How much is 10+10?\")"]},{"cell_type":"code","execution_count":56,"id":"c65f74ca","metadata":{},"outputs":[{"data":{"text/plain":["' The captain of the West Indies team during the first cricket world cup was Clive Lloyd. He was a skilled batsman and led his team to victory with his strong leadership and strategic decisions. He also went on to captain the West Indies team in the next two world cups, winning both of them as well.'"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"Who was the captain of the winning team?\")"]},{"cell_type":"code","execution_count":57,"id":"6eb01645","metadata":{},"outputs":[{"data":{"text/plain":["'  The captain of the losing team, Australia, during the first cricket world cup was Ian Chappell. He was a talented batsman and captain, but unfortunately his team fell short in the final match against the West Indies. However, he went on to captain Australia in later years and achieved great success in the cricket world.'"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["convo.run(\"Who was the captain of the Loosing team?\")"]},{"cell_type":"code","execution_count":58,"id":"a3c251a7","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Human: How much is 10+10?\n","AI:   The answer to 10+10 is 20. This is because 10 plus 10 equals 20. It is a simple arithmetic equation that can be easily solved. Is there anything else you would like to know?\n","Human: Who was the captain of the winning team?\n","AI:  The captain of the West Indies team during the first cricket world cup was Clive Lloyd. He was a skilled batsman and led his team to victory with his strong leadership and strategic decisions. He also went on to captain the West Indies team in the next two world cups, winning both of them as well.\n","Human: Who was the captain of the Loosing team?\n","AI:   The captain of the losing team, Australia, during the first cricket world cup was Ian Chappell. He was a talented batsman and captain, but unfortunately his team fell short in the final match against the West Indies. However, he went on to captain Australia in later years and achieved great success in the cricket world.\n"]}],"source":["print(convo.memory.buffer)"]},{"cell_type":"markdown","id":"mkFhYXKUmnDO","metadata":{"id":"mkFhYXKUmnDO"},"source":["# **08: Document Loaders**\n","\n","- Langchain **Document Loader** converts any type(.pdf,.csv .json etc) format data and converts as **document**\n","- Load the pdf file via langchain framework\n","- https://python.langchain.com/docs/modules/data_connection/document_loaders/\n","- By using this **Document loader** we can read any format document and then split into **chunks** and use **embedding** to convert to vector and save in **vector db** then do **retrival(RAG)** operation"]},{"cell_type":"code","execution_count":59,"id":"-9WXxhHCZtTn","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5785,"status":"ok","timestamp":1697705158337,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"-9WXxhHCZtTn","outputId":"a56d4c31-3e9e-4605-dfd7-bf91800fd276"},"outputs":[],"source":["#!pip install pypdf"]},{"cell_type":"code","execution_count":60,"id":"wqlRJc7DmtwA","metadata":{"id":"wqlRJc7DmtwA"},"outputs":[{"data":{"text/plain":["[Document(page_content='1PrabhavathiBharadwaj\\nBlock490A,#09-211 EmailId:prabhavathi.melady@gmail.com\\nTampinesSt45 ContactNo:+65-91346483\\nSingapore-520490 CertifiedDataScienceProfessional(Edureka)\\nhttps://github.com/PrabhaBharadwaj\\nObjective:\\nTobeassociatedwithaprogressiveorganization,dynamicteamthatgivesanopportunityto\\napplymyknowledge,analyticalskillsandcontributetowardsbuildingdatadrivenorganization.\\nSummary:\\n\\uf0b7DataScienceEnthusiastwhocanunderstandthedriversofbusinessresultsandhow\\ndata/analyticscanacceleratetheoutcomes.\\n\\uf0b7DataAnalystandETLdeveloperwithover8+yearsofexperience(excludingMaternitybreak)in\\nthefieldofDatawarehouse.ExperiencedinBanking,SupplyChainManagementandRetail\\nManagementdomains.\\n\\uf0b7ProficientinSQL,TeradataandworkingexperienceinPython(basics,pandas,numpy),Pyspark,\\nvisualizationtools(i.eh.matplotlib,seaborn)andstatisticsconcepts.\\n\\uf0b7GoodunderstandingonMachineLearning(ML)Supervisedlearning,Unsupervisedlearningand\\nReinforcementLearningandapplystatisticalmodelsinpythonusingmachinelearninglibraries,\\nsuchasscikit-learnandstatsmodels.\\n\\uf0b7ExperiencedatcreatingRegressionmodels,ClassificationModels,Clusteringmodelsand\\nEnsembletechniques(Bagging,Boosting)todeliverinsightsandimplementaction-oriented\\nsolutionstocomplexbusinessproblemstargetingcustomers.\\n\\uf0b7AppliedmachinelearningalgorithmssuchasLinearregressionmodel,Logisticregressionmodel,\\nDecisionTree,RandomForestmodel,AdaBoost,K-Nearest-Neighbour(KNN)Naive-Bayes,Scalar\\nVectorMachine(SVM),K-meanclustering,Hierarchicalclustering.BasicknowledgeonAssociation\\nRuleMining(AprioriAlgorithm)andRecommendationsystem(UBCFandCBCF)\\n\\uf0b7WorkedextensivelyonFeatureEngineering,FeatureSelectiontechniques(Dimensionality\\nReductionality,PCAandLDAalgorithms)\\n\\uf0b7HyperparametertuningusingGridsearchCVandRandomSearchCVtechniques.\\n\\uf0b7BasicknowledgeonTimeSeriesAnalysis(TSA)ARMAModel,NLP(NaturalLanguageProcessing)\\ntechniquesandDeepLearning(Tensorlfow,Keras:ANN,CNN,RNN)\\n\\uf0b7ExperiencewithCI/CD,MLOpsandtoolslikeGitHub(CodevesrionControl),DVC(DataVersion\\nControl),MLFlow(modeltrackinganddeployment)\\n\\uf0b7UnderstandingonWebappFrameworklikeFlask,Streamlit,FastAPI,PyWebIO,FlasggerforAPI\\ndevelopment.KnowledgeonwebscrappingusingBeautifulSouplibrary.\\n\\uf0b7KnowledgeonCloudPlatformforMLdeployment(Heroku,Azure,AWSEC2,GoogleCloud\\nplatform)\\n\\uf0b7KnowledgeofHadoop(HDFS,YARN,MapReduce,Hive,Pig,Hbase,Kafka)andSpark\\necosystem(SparkSQL,Pyspark,Streaming,MLlib).Experiencehadlinglarge,distributeddatasets\\n(Ingestion,wrangling,analysing,insightgeneration).', metadata={'source': './Resume_Prabha.pdf', 'page': 0}),\n"," Document(page_content='2\\uf0b7Submittedmultipleprediction,forecastingcasestudyexerciseswithdifferentmodelsinEdureka.\\nWorkedonPharma,Socialmedia,RetailProjectsasmainprojectsinEdureka.\\n\\uf0b7Dynamic,hardworking,adaptable,self-directedandmotivatedindividualwithemphasison\\ndeliveringqualityservices.\\n\\uf0b7Goodcommunicationandleadershipskills,stronganalytical,designskills,problemsolving\\nabilitiesandco-ordinationwithoffshoreteams.\\n\\uf0b7Recognizedwith‘OS’forextensivequalitydeliverablesinBIprojects.\\nCompanyProfile:\\nCompanyName Designation Duration\\nNA DataScientistEnthusiastFromNov2018\\nNote:TakenMaternitybreakinNov2018\\nandstuckinIndiamorethan2yrdueto\\nborderrestriction,NowinSG.Duringthis\\nperiodSelfpracticingDataScience\\nStandardCharteredBank\\nSingaporeSeniorDataAnalyst Sept2016toOct2018\\nDBSAsiaHubSingapore DataAnalyst Dec2014toSept2016\\nTescoHSCBangalore SeniorSoftwareEngineerDec2013toOct2014\\nHCLTechnologiesLtd.forIntel SeniorSoftwareEngineerJuly2012toDec2013\\nHCLTechnologiesLtd.forOCBC SoftwareEngineer July2010toJuly2012\\nEducationProfile,Certificates:\\n\\uf0b7BachelorofEngineering(Electronics&Communication,BMSCE,Bangalore)\\n\\uf0b7PythonforDataScienceProfessionalcertifiedwithGradeA(Edureka)\\n\\uf0b7BigdataHadoopCerification(Edureka)\\nAsa‘DataAnalyst’:\\nRolesandResponsibilitiesPerformed:\\n\\uf0b7ExtensivelyworkedonRequirementgathering,DataAnalysis,Design,Development,Testingand\\nImplementation.ProficientinEnterpriseDatawarehouseandETLconcepts.Experienceworking\\ninDatastage,AbInitio,FSLDM,BusinessObjectsandCognos.\\n\\uf0b7Functionalspecificationdocumentpreparation\\n\\uf0b7Project/UR(UserRequests)endorsementfromDatawarehouseArchitecturalteam\\n\\uf0b7Involvementindatamodelingdiscussions\\n\\uf0b7Sourcesystemsanalysisandsourcedatavalidation,dataprofiling.Sourcetotargetdatamapping\\npreparation\\n\\uf0b7Analyticaldatamartmappingi.econvertingbusinessrulesintotechnicalrules\\n\\uf0b7WorkedcloselywithProjectTeam,SMEandInternalBusinessPartnerstoclarifybusiness\\nrequirements.Involvedinbusiness/clientteammeetings.\\n\\uf0b7PerformingImpactanalysis\\n\\uf0b7SITandUATtestcasespreparation', metadata={'source': './Resume_Prabha.pdf', 'page': 1}),\n"," Document(page_content='3\\uf0b7SITandUATdatavalidation,dataintegritytestinganduserssignoffco-ordination\\n\\uf0b7Regressiontesting/validationsupport.\\n\\uf0b7Liveverificationandbusinesssignoffsupport\\n\\uf0b7InvolvedinStatementofWork(SOW)processforfewinitiatives\\nAsa‘ETLDeveloper’:\\nMainRolesandResponsibilitiesPerformed:\\n\\uf0b7Performingdataprofilingforvarioussourcesystems\\n\\uf0b7UpstreamandDownstreamETLdevelopmenti.eSourcetoEDWandEDWtoDataMart\\ndevelopment\\n\\uf0b7ExperiencedindevelopingtransformationpackagesinOracleDataIntegrator(ODI)\\n\\uf0b7DevelopingBTEQandTPTscriptfordatatransfer\\n\\uf0b7Unittesting,SIT,UATandRegressiontestingsupport\\n\\uf0b7Deploymentsupport\\n\\uf0b7ExperiencedinjobsschedulingtoolslikeControl-MandTWS\\n\\uf0b7WorkedonPerformancetuningtoreducetheloadtime\\n\\uf0b7PreparationofNFRdocuments\\n\\uf0b7DocumentspreparationforBAUclearanceforproductionrollout\\nProjects\\nFewDataScienceProjects:\\n\\uf0b7Classification:\\noEarlyNPAdetectionforloanproduct\\noSpam-HamMailPrediction-NLP\\noJobTypepredictionusingNLPfromJobTitle\\noChemicalBioDegradableMaterialPrediction\\noCancerPrediction\\noBankNoteAuthenticationcheck\\n\\uf0b7Regression:\\noHousepricePrediction-KaggleCompetition\\noSalaryPrediction\\noFyntraRetailShopSalesanalysisandhelpsbusinessPlanning\\noCarPriceprediction\\n\\uf0b7Clustering:\\noClusteredthecountriesbasedonvarioussalesdataprovidedtousacrossyearsby\\nmultinationalexporter,totargetspecificcountriesformoresales\\noCustomerSegmentationbasedonuserDataprovidedbyMall\\noLithionpowerprovideselectricvehicle(e-vehicle)batteries,thisplanningtoIncentivize\\nDriver’sbasedontheirDrivingdetails\\n\\uf0b7Forecasting:\\noSeaplaneforecastdemanddetailstoSeaPortbusiness\\nSCB:\\n\\uf0b7CAB(CrossBorderAssetBooking)\\n\\uf0b7PEMBROKE\\nDBS:', metadata={'source': './Resume_Prabha.pdf', 'page': 2}),\n"," Document(page_content='4\\uf0b7FATCA&CRSReporting\\n\\uf0b7InternetBankingandMobileBankingDigitalOn-Boarding\\n\\uf0b7CampaignFulfillmentReporting\\n\\uf0b7RM’sIncentiveCalculation\\nTesco:\\n\\uf0b7IntelligentTradingPlatform–CompetitorReactivePricing\\n\\uf0b7NetezzaToExadataMigrationProject\\nIntel:\\n\\uf0b7PerfectOrderMetrics(POM)\\n\\uf0b7Strom\\nOCBC:\\n\\uf0b7GlobalMISMalaysia,OCBCMalaysiaRegulatoryReporting(MYRR)\\nPersonalDetails\\nNationality Indian\\nSex Female\\nMaritalStatus Married\\nCurrentLocation Singapore(DP)\\nDeclaration\\nIherebydeclarethattheinformationfurnishedaboveistruetothebestofmyknowledge.\\nPrabhavathiBharadwaj', metadata={'source': './Resume_Prabha.pdf', 'page': 3})]"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# .pdf file Loader\n","from langchain.document_loaders import PyPDFLoader\n","\n","#loader = PyPDFLoader(\"/content/my_paper.pdf\")\n","loader = PyPDFLoader(\"./Resume_Prabha.pdf\")\n","pages = loader.load()\n","pages"]},{"cell_type":"code","execution_count":61,"id":"XGTtdS26mt0_","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":687,"status":"ok","timestamp":1697705287553,"user":{"displayName":"colab0 ineuron","userId":"16851312232179065356"},"user_tz":-360},"id":"XGTtdS26mt0_","outputId":"790db9e9-b1bf-46c2-bc38-299e8afa17b8"},"outputs":[{"data":{"text/plain":["[Document(page_content='# Basic Langchain Code\\n\\nLangChain is a framework for developing applications powered by language models.\\n\\n- GitHub: https://github.com/hwchase17/langchain\\n- Docs: https://python.langchain.com/en/latest/index.html\\n\\n- Holds Calling OpenAI API, HuggingFace API, Gemini(Google) API Connection examples from **Langchain**\\n\\n### Overview:\\n\\n- **Installation**\\n- **Setup the environment**\\n- **LLMs** (Connct to different LLM using API_KEY via langchain)\\n  - Tried **OPENAI** and **HuggingFace** LLM model using API_Keys\\n- **Prompt Templates** (It provides template for llm prompt)\\n  - Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM.\\n- **Chains** (Chains of prompt)\\n\\n  - Combine LLMs and Prompts in multi-step workflows\\n  - Provided example Without Chain running one after the other\\n    - **5.1. Simple Sequential Chain**\\n      - Combine Multiple PromptTemplates\\n      - The output from the **first PromptTemplate is passed to the next PromptTemplate** as input\\n      - Only final prompt output displayed\\n    - **5.2. Sequential Chain**\\n      - Its same as **simplesequentialchain** just difference is here, we define **output_key**\\n      - If we want **all** the prompt output then use **Sequential Chain**\\n\\n- **Agents** (With the help of agents, we are accessing Tools) and Tools (Tools which we are using Serpapi,wikipedia)\\n\\n  - Whenever we want **real time data** which is not there in LLM(Which trained using old data), then only we use Agents. If data is fixed then we use RAG\\n  - Agent will conenct with **external tools** and it will use LLM reasoning capabilities\\n  - All the tools like **Google Search Tool and Math Tool are available as part of LangChain and you can configure agent, so agent is nothing but using all these tools and LLM reasoning capabilities** to perform a given task\\n  - To access Google Search Results in Real Time we use **serpapi**\\n\\n    - **Tool:** A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\\n    - **LLM:** The language model powering the agent.\\n    - **Agent:** The agent to use.\\n\\n  - **6.1 AGENT: serpapi(Google serach) and llm-math tool**\\n  - **6.2 AGENT: Wikipedia and llm-math tool**\\n\\n- **Memory** (Remembering Chat History) - We can attach memory to remember all previous conversation\\n\\n      - **7.1 ConversationBufferMemory**\\n          - We can attach memory to remember all previous conversation\\n\\n  Conversation buffer memory goes growing endlessly, It takes huge space\\n\\n      - **7.2 ConversationChain**\\n\\n      - This is same like Conversation buffer memory, It keep on adds content/ q&a respnse to memory, keeps growing\\n      - It behaves like AI and human conversation\\n\\n\\n      - **7.3 ConversationBufferWindowMemory**\\n\\n      - Conversation buffer memory and ConversationChain goes growing endlessly, It takes huge space\\n      - It  Just remember last 5 Conversation Chain based on we set parameter k=5\\n      - It Just remember last 10-20 Conversation Chain based on we set parameter k=10/20\\n\\n- **Document Loaders** (Load our PDF file)\\n\\n  - Load the pdf file via langchain framework\\n  - https://python.langchain.com/docs/modules/data_connection/document_loaders/\\n\\n- **Indexes**\\n', metadata={'source': './README.md'})]"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["# .md file Loader\n","from langchain_community.document_loaders import TextLoader\n","\n","loader = TextLoader(\"./README.md\")\n","pages = loader.load()\n","pages"]},{"cell_type":"markdown","id":"a1cdb08d","metadata":{},"source":["> This pdf data we will convert to vector using vectorDB in future class"]},{"cell_type":"markdown","id":"2efed68d","metadata":{},"source":["# **END**"]},{"cell_type":"code","execution_count":null,"id":"FSnv3gV3muBm","metadata":{"id":"FSnv3gV3muBm"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"uO4dx_pNmuFt","metadata":{"id":"uO4dx_pNmuFt"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":5}
