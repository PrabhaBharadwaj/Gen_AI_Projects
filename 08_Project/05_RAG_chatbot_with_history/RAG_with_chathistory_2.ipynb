{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Memory\n",
        "- Invoke Openai model\n",
        "- Create simple chain and invoke\n",
        "- Add chat history using: **ChatMessageHistory**\n",
        "- Add chat history using: **RunnableWithMessageHistory**\n",
        "- Langchain OpenAI and use ConversationChain\n",
        "- LangChain - ConversationBufferMemory\n",
        "\n"
      ],
      "metadata": {
        "id": "TdKG_EDTawzN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O625tmIiarjO",
        "outputId": "a9aa0892-6e4c-4323-f7e0-583aa7a86a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.5/409.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-community langchain-openai -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage  # new message class in langchain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# iNVOKE OPENAPI KEY\n",
        "OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY']=OPENAI_API_KEY\n"
      ],
      "metadata": {
        "id": "bDnt7sBubYiu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Invoke Openai model"
      ],
      "metadata": {
        "id": "qulve6nSC58U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INVOKE OPENAPI MODEL\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "\n",
        "#OPENAI ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\n",
        "        \"system\",\n",
        "        \"you are a helpful assistant. Answer all the question to the best of your ability.\"\n",
        "    ),\n",
        "    MessagesPlaceholder(variable_name = \"messages\"),  # User Q saved in messages parameter\n",
        "]\n",
        ")"
      ],
      "metadata": {
        "id": "xZTkrkzNbQNE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create simple chain and invoke"
      ],
      "metadata": {
        "id": "IbNTdgWuCx6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain =  prompt | llm\n",
        "\n",
        "resposne = chain.invoke(\n",
        "    {\n",
        "        # This messsages parameter should be same as above prompt parameter\n",
        "        # Here conversational message sample kept inside messagees\n",
        "       \"messages\": [\n",
        "\n",
        "                   HumanMessage(\n",
        "                       content= \"translate this given sentence from english to french: I LOVE AI.\"  # Human Q\n",
        "                   ),\n",
        "                   AIMessage(content= \"J'adore la AI.\"),      # AI Response\n",
        "\n",
        "                   HumanMessage(content= \"what did you just say?\"),  # Human Q\n",
        "\n",
        "       ] ,\n",
        "\n",
        "    }\n",
        ")\n",
        "\n",
        "resposne.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Gvz8k8dmd_6y",
        "outputId": "8cd693e8-6f37-4853-f685-7466e2753552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I said \"I love AI\" in French.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add chat history using: ChatMessageHistory"
      ],
      "metadata": {
        "id": "EOnUsOPwEEKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.memory import ChatMessageHistory"
      ],
      "metadata": {
        "id": "L5fyhefUeMkp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history=ChatMessageHistory()\n",
        "demo_chat_history.add_user_message(\"translate this given sentence from english to french: I LOVE AI.\")\n",
        "demo_chat_history.add_ai_message(\"J'adore la AI.\")\n",
        "demo_chat_history.messages  #displaay message history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxLeaC71gRxR",
        "outputId": "eb315111-5333-45f6-8133-beebd76ce59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='translate this given sentence from english to french: I LOVE AI.'),\n",
              " AIMessage(content=\"J'adore la AI.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatMessageHistory - Example 1"
      ],
      "metadata": {
        "id": "Wbgb8xJjFdlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history=ChatMessageHistory()\n",
        "input1=\"who is a prime minister of india?\"\n",
        "demo_chat_history.add_user_message(input1)\n",
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj1jatjzg7k5",
        "outputId": "2dcc2c31-20b0-444a-fcb4-c7af869a2cbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='who is a prime minister of india?')]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resposne=chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")\n",
        "\n",
        "resposne.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hYp7_AkahASE",
        "outputId": "6052b1cf-0813-4091-ff3d-5a02b8c35011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'As of my last update, the Prime Minister of India is Narendra Modi. He has been in office since May 2014.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatMessageHistory - Example 2"
      ],
      "metadata": {
        "id": "RXo1KD4NFjWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "demo_chat_history.add_ai_message(resposne)\n",
        "input2=\"what did i ask to you just now?\"\n",
        "demo_chat_history.add_user_message(input2)\n",
        "demo_chat_history.messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qhJgQEzhkxC",
        "outputId": "30e4ccdf-f4fc-46d8-e126-58e4e3e0e001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='who is a prime minister of india?'),\n",
              " AIMessage(content='As of my last update, the Prime Minister of India is Narendra Modi. He has been in office since May 2014.', response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 36, 'total_tokens': 62}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-b2a844d2-f60c-48fa-83df-ebd2f7381a48-0', usage_metadata={'input_tokens': 36, 'output_tokens': 26, 'total_tokens': 62}),\n",
              " HumanMessage(content='what did i ask to you just now?')]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response2 = chain.invoke(\n",
        "    {\n",
        "        \"messages\": demo_chat_history.messages\n",
        "    }\n",
        ")\n",
        "response2.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OK-G9O5lh2AB",
        "outputId": "d0bf95ea-4ca5-47bb-d5bb-5383688e5519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You asked about the current Prime Minister of India.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add chat history using: RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "eHfd2v2XmCAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatMessageHistory -  with prompt 1"
      ],
      "metadata": {
        "id": "R3zNxbnAFmx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt2 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain2 = prompt | llm"
      ],
      "metadata": {
        "id": "tMq_VhHCo0sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ChatMessageHistory -  with prompt 2"
      ],
      "metadata": {
        "id": "V58N1K7iF2aX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt3 = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability.\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{chat_history}\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt3 | llm\n"
      ],
      "metadata": {
        "id": "IU38-INhwopF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "demo_chat_history2 = ChatMessageHistory()\n",
        "\n",
        "chain_with_message_history = RunnableWithMessageHistory(\n",
        "                              chain,\n",
        "                              lambda session_id: demo_chat_history2,\n",
        "                              input_message_key = \"input\",\n",
        "                              history_message_key =\"chat_history\",\n",
        "                              )\n",
        "\n",
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"Translate this sentence from English to French: I love programming.\"},\n",
        "    {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDXkULIXw043",
        "outputId": "c9e62313-8ea4-4b75-9477-6f5c97c7c775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.core:Parent run ae2886be-edee-4d33-a82e-f286775c908f not found for run eb8688b7-fb37-4e55-9435-bd94826fa54b. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The translation of \"I love programming\" from English to French is \"J\\'adore la programmation.\"', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 45, 'total_tokens': 67}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-d248d27c-313a-4713-bc03-c3c1883b7e3a-0', usage_metadata={'input_tokens': 45, 'output_tokens': 22, 'total_tokens': 67})"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_message_history.invoke(\n",
        "    {\"input\": \"What did I just ask you?\"}, {\"configurable\": {\"session_id\": \"unused\"}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8ZfiLzdpEI2",
        "outputId": "791bd8c8-b06d-4fee-bc8a-c94fe1305986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.tracers.core:Parent run 9c0a93c3-438a-4b5f-a3ec-80a93574a7a3 not found for run 3525a47b-059a-4429-8a57-fa9707750ec6. Treating as a root run.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='You just asked me to translate the sentence \"I love programming\" from English to French.', response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 210, 'total_tokens': 228}, 'model_name': 'gpt-3.5-turbo-1106', 'system_fingerprint': 'fp_3d37c73133', 'finish_reason': 'stop', 'logprobs': None}, id='run-6bcf552b-19c2-4414-989d-1ce291d0c664-0', usage_metadata={'input_tokens': 210, 'output_tokens': 18, 'total_tokens': 228})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langchain OpenAI and use ConversationChain"
      ],
      "metadata": {
        "id": "K9ZpYaI5mgZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI"
      ],
      "metadata": {
        "id": "Tzc8ItzeqqzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''llm = OpenAI(\n",
        "\ttemperature=0,\n",
        " model_name=\"gpt-3.5-turbo-1106\"\n",
        ")\n",
        " '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4j8Z8cT1rt7g",
        "outputId": "bbcc02e3-20ab-4ace-a7cf-9b956da1d444"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_community/llms/openai.py:1072: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "conversation = ConversationChain(llm=llm)\n",
        "print(conversation.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ROVWwlLsHlH",
        "outputId": "780c8372-c3d2-4b75-ac2c-04dc5a4f7c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"i am interested in exploring the potential of large language model with external knowledge\"\n",
        "question2=\"i would like to analysis all the different possibility, what can you think of?\"\n",
        "question3=\"which data source tyoe could be used to give context to the model?\""
      ],
      "metadata": {
        "id": "th4Pp8bysQvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangChain - ConversationBufferMemory"
      ],
      "metadata": {
        "id": "t7o4BlYjRLah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
        "\n",
        "conversation_buf = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory= ConversationBufferMemory()\n",
        "\n",
        ")\n",
        "\n",
        "conversation_buf(\"good morning my AI\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhGlC_etwTy",
        "outputId": "3b9ab0ca-ba70-4845-b9bd-62009cf11319"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'good morning my AI',\n",
              " 'history': '',\n",
              " 'response': 'Good morning! How are you today? Did you sleep well? The weather outside is currently 65 degrees Fahrenheit with a 30% chance of rain. How can I help you?'}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### function to get total toke count"
      ],
      "metadata": {
        "id": "maNNbayITJjF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks import get_openai_callback\n",
        "\n",
        "def count_tokens(chain,query):\n",
        "  with get_openai_callback() as cb:\n",
        "    result= chain.run(query)\n",
        "    print(f\"totla no of token is {cb.total_tokens}\")\n",
        "  return result"
      ],
      "metadata": {
        "id": "lQ2nwZdvup3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question1=\"i am interested in exploring the potential of large language model with external knowledge\"\n",
        "\n",
        "count_tokens(conversation_buf,question1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "54oRwJ3du-8k",
        "outputId": "4365af4b-15a6-4f02-ad73-f2d8c8a06412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 233\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"That's a fascinating topic! Large language models with external knowledge have shown great potential in improving natural language understanding and generation. By incorporating external knowledge sources such as encyclopedias, databases, and websites, these models can provide more accurate and informative responses to user queries. They can also better understand the context of a conversation and provide more contextually relevant information. There are various research papers and articles available that delve into the details and potential applications of these models. Is there a specific aspect of this topic that you would like to explore further?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-cax7oQvS-A",
        "outputId": "4b85fd19-fb8f-4cce-8c1a-c06a95ffb983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i am interested in exploring the potential of large language model with external knowledge',\n",
              " 'history': \"Human: good morning my AI\\nAI: Good morning! How are you today? Did you sleep well? The weather outside is currently 65 degrees Fahrenheit with a 30% chance of rain. How can I help you?\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: That's a fascinating topic! Large language models with external knowledge have shown great potential in improving natural language understanding and generation. By incorporating external knowledge sources such as encyclopedias, databases, and websites, these models can provide more accurate and informative responses to user queries. They can also better understand the context of a conversation and provide more contextually relevant information. There are various research papers and articles available that delve into the details and potential applications of these models. Is there a specific aspect of this topic that you would like to explore further?\",\n",
              " 'response': 'I see, you are interested in exploring the potential of large language models with external knowledge. These models have shown great potential in improving natural language understanding and generation. By incorporating external knowledge sources such as encyclopedias, databases, and websites, these models can provide more accurate and informative responses to user queries. They can also better understand the context of a conversation and provide more contextually relevant information. There are various research papers and articles available that delve into the details and potential applications of these models. Is there a specific aspect of this topic that you would like to explore further?'}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question2=\"i would like to analysis all the different possibility, what can you think of?\"\n",
        "conversation_buf(question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYf3GnPxvlKI",
        "outputId": "efa67efa-f2f5-40ac-aa58-e43486474ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'i would like to analysis all the different possibility, what can you think of?',\n",
              " 'history': \"Human: good morning my AI\\nAI: Good morning! How are you today? Did you sleep well? The weather outside is currently 65 degrees Fahrenheit with a 30% chance of rain. How can I help you?\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: That's a fascinating topic! Large language models with external knowledge have shown great potential in improving natural language understanding and generation. By incorporating external knowledge sources such as encyclopedias, databases, and websites, these models can provide more accurate and informative responses to user queries. They can also better understand the context of a conversation and provide more contextually relevant information. There are various research papers and articles available that delve into the details and potential applications of these models. Is there a specific aspect of this topic that you would like to explore further?\\nHuman: i am interested in exploring the potential of large language model with external knowledge\\nAI: I see, you are interested in exploring the potential of large language models with external knowledge. These models have shown great potential in improving natural language understanding and generation. By incorporating external knowledge sources such as encyclopedias, databases, and websites, these models can provide more accurate and informative responses to user queries. They can also better understand the context of a conversation and provide more contextually relevant information. There are various research papers and articles available that delve into the details and potential applications of these models. Is there a specific aspect of this topic that you would like to explore further?\",\n",
              " 'response': 'Some potential areas of exploration could include the impact of different types of external knowledge sources on the performance of large language models, the development of more efficient algorithms for integrating external knowledge, and the ethical considerations of using external knowledge in language generation. Additionally, you could also explore the potential applications of large language models with external knowledge in various industries such as healthcare, finance, and customer service. There are also opportunities to investigate the potential limitations and challenges of these models, such as biases in the external knowledge sources and the potential for misinformation. These are just a few possibilities, and there are many more aspects to consider in exploring the potential of large language models with external knowledge.'}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(conversation_buf,question2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "P1YQb48UvnVY",
        "outputId": "e98607f0-5438-45b5-dd94-bdbad63f8bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "totla no of token is 673\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Some potential areas of exploration could include the impact of different types of external knowledge sources on the performance of large language models, the development of more efficient algorithms for integrating external knowledge, and the ethical considerations of using external knowledge in language generation. Additionally, you could also explore the potential applications of large language models with external knowledge in various industries such as healthcare, finance, and customer service. There are also opportunities to investigate the potential limitations and challenges of these models, such as biases in the external knowledge sources and the potential for misinformation. These are just a few possibilities, and there are many more aspects to consider in exploring the potential of large language models with external knowledge.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some potential areas of exploration could include the impact of different types of external knowledge sources on the performance of large language models, the development of more efficient algorithms for integrating external knowledge, and the ethical considerations of using external knowledge in language generation. Additionally, you could also explore the potential applications of large language models with external knowledge in various industries such as healthcare, finance, and customer service. There are also opportunities to investigate the potential limitations and challenges of these models, such as biases in the external knowledge sources and the potential for misinformation. These are just a few possibilities, and there are many more aspects to consider in exploring the potential of large language models with external knowledge."
      ],
      "metadata": {
        "id": "kG0jJPrMv4hl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hIqJqOIAvyGB",
        "outputId": "032dc047-670c-48f1-bdc4-a7489f3a0d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'which data source tyoe could be used to give context to the model?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation_buf(question3)['response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "UOINeOzOwEJ8",
        "outputId": "518d8a78-1594-4bd8-fbc5-edf93d1ac183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Different types of data sources that could be used to give context to the model include structured databases, unstructured text from sources like news articles or books, knowledge graphs, and domain-specific knowledge bases. These data sources can provide the model with a rich understanding of various topics and domains, allowing it to generate more accurate and contextually relevant responses. Additionally, real-time data from sources such as social media or online forums can also be used to provide up-to-date context for the model. Each type of data source has its own strengths and limitations, and exploring the potential of each could provide valuable insights into enhancing the capabilities of large language models with external knowledge.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment\n",
        "- take atleast 5 data source(1 ppt, 1 weblodaer, 1 pdf, 1txt, 1csv)\n",
        "- maintain one knowledge base(mongodb, astradb , pineconde, weviate)\n",
        "- user will ask you have to provide answer based on the asked question using this know;ledge base\n",
        "- handle the comman question as well like hi hello how are you good evening good morning etc.\n",
        "- you have to mention the complete memory of the conversation here the threshold is 10\n",
        "- then create a UI for you bot\n",
        "\n",
        "https://www.youtube.com/@sunnysavita10/videos\n"
      ],
      "metadata": {
        "id": "lwLzzCqZxec0"
      }
    }
  ]
}