{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1PVYU0jCcMaYoB5rvI6-lJvfBN9KYm4Ka","timestamp":1708156241438}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"d2aa59a0138f49b8b8eb7e1d6bec6c12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_532f06d941d744a593858b832ae1b07e","IPY_MODEL_65bbc56e51274f01a5f5fbb73c71ca72","IPY_MODEL_e76c85a6716a4c66bf9f5da6e0c9febb"],"layout":"IPY_MODEL_cc56203c0c7f474b93c4b427ae17c802"}},"532f06d941d744a593858b832ae1b07e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_029d6640b1ef4bda9236f3daaad29304","placeholder":"​","style":"IPY_MODEL_f94896de21bb48b2a8a63ff16c137a72","value":"model.safetensors: 100%"}},"65bbc56e51274f01a5f5fbb73c71ca72":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_be7ade465599475f807bf76d4e89770d","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_15c122eeba0f46e4bd466a35d80bc468","value":440449768}},"e76c85a6716a4c66bf9f5da6e0c9febb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d50d04afe9cb40eda678e1d5fdfc320f","placeholder":"​","style":"IPY_MODEL_1c442860c9ca403381bc877dcfba3fe0","value":" 440M/440M [00:04&lt;00:00, 141MB/s]"}},"cc56203c0c7f474b93c4b427ae17c802":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"029d6640b1ef4bda9236f3daaad29304":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f94896de21bb48b2a8a63ff16c137a72":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"be7ade465599475f807bf76d4e89770d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15c122eeba0f46e4bd466a35d80bc468":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d50d04afe9cb40eda678e1d5fdfc320f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c442860c9ca403381bc877dcfba3fe0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **Hugging Face Transformer Model**\n","- Download **BERT** **transformer** based model from hugging face\n","- Predict **NER(Named entity Reconition)** in the data Using pretrained model\n","- Do the Fine tuning of that pretrained model using our dataset **conll2003**\n","- Do the Validation(Evalution) during Fine tuning\n","- Use the Fine tuned moddel and do the inferencing on new data to identify NER in that data\n","- **CoNLL-2003** is a **named entity recognition** dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition.\n","- Here **BERT LLM model** used"],"metadata":{"id":"dQmxsywgJQqw"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"yVE30EMThWPj","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1708159268579,"user_tz":-480,"elapsed":5,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"6c82cb0d-373b-417a-f9c4-4882e055e4ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'!pip install transformers\\n!pip install datasets\\n!pip install tokenizer\\n!pip install seqeval\\n\\n#either you can use above statement or use single command\\n# Install\\n!pip install transformers datasets tokenizers seqeval -q\\n\\n!pip install transformers[torch]\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["\"\"\"!pip install transformers\n","!pip install datasets\n","!pip install tokenizer\n","!pip install seqeval\n","\n","#either you can use above statement or use single command\n","# Install\n","!pip install transformers datasets tokenizers seqeval -q\n","\n","!pip install transformers[torch]\n","\"\"\""]},{"cell_type":"code","source":["import datasets\n","import numpy as np\n","from transformers import BertTokenizerFast\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification"],"metadata":{"id":"IkF5laABlwL9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Downloading Hugging face **conll2003 - NER** data\n","\n","- **CoNLL-2003** is a **named entity recognition** dataset released as a part of CoNLL-2003 shared task: language-independent named entity recognition.\n","\n"],"metadata":{"id":"mCA_Vf1UHTbz"}},{"cell_type":"code","source":["conll2003 = datasets.load_dataset(\"conll2003\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOSMUGPtnkH1","executionInfo":{"status":"ok","timestamp":1708159293731,"user_tz":-480,"elapsed":8095,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"e8193632-8a57-41ee-c87e-9d89ee0c007f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["conll2003"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmNlLA8SofOf","executionInfo":{"status":"ok","timestamp":1708159293732,"user_tz":-480,"elapsed":34,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"9cdc7b09-1e71-4ced-b2d3-fea695a6ed28"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 14041\n","    })\n","    validation: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3250\n","    })\n","    test: Dataset({\n","        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","        num_rows: 3453\n","    })\n","})"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## Here Feature will have list of items, id, token,pos_tags, chunk_tags, ner_tags\n","- Data has 3 set of data.\n","- id is unique no to each row. its like rownumber\n","- Here Tokens will have actual data as a list of token,\n","- pos_tags will positional tag of that token data\n","- ner_tags - > Will have named entity for each token"],"metadata":{"id":"aCu2QljRIVt_"}},{"cell_type":"code","source":["conll2003[\"train\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Q5bDAubptQi","executionInfo":{"status":"ok","timestamp":1708159293732,"user_tz":-480,"elapsed":29,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"ce541a09-e78f-4d3f-99a0-bfa5e1fe3416"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","    num_rows: 14041\n","})"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["conll2003[\"train\"][0]\n","#1st Line of Data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a39FQ94Kp0QG","executionInfo":{"status":"ok","timestamp":1708159293733,"user_tz":-480,"elapsed":26,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"4ab8903a-4cc1-4b00-d90b-734b4c960692"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["conll2003[\"train\"][1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iR8tTo6iqa4o","executionInfo":{"status":"ok","timestamp":1708159293733,"user_tz":-480,"elapsed":21,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"5099c377-d776-4e0a-bf3e-f58c825e4e3a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '1',\n"," 'tokens': ['Peter', 'Blackburn'],\n"," 'pos_tags': [22, 22],\n"," 'chunk_tags': [11, 12],\n"," 'ner_tags': [1, 2]}"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["conll2003[\"train\"][0]['tokens']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M17BoHAzK5Fj","executionInfo":{"status":"ok","timestamp":1708159293733,"user_tz":-480,"elapsed":17,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"9848c1b4-48ff-40d2-8d97-3acfd9839fca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["conll2003[\"train\"][0]['ner_tags']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2grhZt6qLWzW","executionInfo":{"status":"ok","timestamp":1708158461241,"user_tz":-480,"elapsed":15,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"ce34c7ae-a024-4340-fe61-c28d77231733"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3, 0, 7, 0, 0, 0, 7, 0, 0]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["conll2003[\"train\"][14040]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhHZ8mxgM7W8","executionInfo":{"status":"ok","timestamp":1708159293734,"user_tz":-480,"elapsed":14,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"04782678-7249-4122-8fd1-3638674be75b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '14040',\n"," 'tokens': ['Swansea', '1', 'Lincoln', '2'],\n"," 'pos_tags': [21, 11, 22, 11],\n"," 'chunk_tags': [11, 12, 12, 12],\n"," 'ner_tags': [3, 0, 3, 0]}"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["conll2003[\"train\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssze1YugL9yU","executionInfo":{"status":"ok","timestamp":1708159293734,"user_tz":-480,"elapsed":10,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"ca4d13c9-4383-466c-c22d-c55064d388e3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset({\n","    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","    num_rows: 14041\n","})"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["#These are distinct NER taags, which is going to be our output labels\n","conll2003[\"train\"].features['ner_tags']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I75eeKXcp4qx","executionInfo":{"status":"ok","timestamp":1708159295155,"user_tz":-480,"elapsed":11,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"fc5c122e-028f-4d3a-dbc0-5525b81d3fd4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["#These are distinct NER taags\n","conll2003[\"train\"].features['id']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-3SM0M3zMARa","executionInfo":{"status":"ok","timestamp":1708159297111,"user_tz":-480,"elapsed":3,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"8b64ec48-5784-40dd-bd6b-50c57c96e568"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Value(dtype='string', id=None)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# This gives description of that dataset, which NER specific dataset\n","conll2003[\"train\"].description"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":191},"id":"WEwLrLrxqQRC","executionInfo":{"status":"ok","timestamp":1708159415754,"user_tz":-480,"elapsed":501,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"0b7b3bd0-929c-470f-9a2a-780036402e16"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["## Use Pretrained Model - **bert-base-uncased**'s Tokenizer\n","- Here uncased means whole data trained on lower case . English and english are same\n","- BERT base model (uncased) - Pretrained model on English language using a **masked language modeling (MLM)** objective\n","- **BertTokenizerFast** class has both way tokenization classes. 1 converts string to token, token to string\n","- **BertTokenizerFast.convert_tokens_to_string** converts ids to string, not tokens to string"],"metadata":{"id":"z2otxToIPn46"}},{"cell_type":"code","source":["tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"],"metadata":{"id":"HmxRWM5Orijk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conll2003['train'][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StnO3YtTxxCv","executionInfo":{"status":"ok","timestamp":1708160052751,"user_tz":-480,"elapsed":16,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"708ee925-6a3b-4f60-feaf-75b920045c5f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["#Taking 1 row from dataset\n","example_text = conll2003['train'][0]\n","example_text"],"metadata":{"id":"iyitpGdMyF0h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708160142004,"user_tz":-480,"elapsed":532,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"aaf1e19e-fa4c-4289-c9a8-a931f77bbd24"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["example_text[\"tokens\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4rJLE_WykFj","executionInfo":{"status":"ok","timestamp":1708160145623,"user_tz":-480,"elapsed":10,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"dd4684dc-e55a-4290-e703-186074da98a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["### Apply this BERT tokenizer on 1 row for testing\n","- It converts Data to integer, based on BERTS Tokenizer dictonary\n","- Once we apply tokenizer, it adds CLS(101) and SEPERATOR/SEP(102) on whole corpus, Its dummy"],"metadata":{"id":"099-vr9sOHzC"}},{"cell_type":"code","source":["tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True)"],"metadata":{"id":"te0BldE9yWXC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_input[\"input_ids\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iBePj4qVypbq","executionInfo":{"status":"ok","timestamp":1708153793973,"user_tz":-330,"elapsed":52,"user":{"displayName":"sunny savita","userId":"10135199589065204167"}},"outputId":"6c15eb69-61f3-414e-985c-5369be17c768"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":[" tokenizer(['Prabha'], is_split_into_words=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPtEhmOvSOGA","executionInfo":{"status":"ok","timestamp":1708160353278,"user_tz":-480,"elapsed":591,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"e34d72ed-a309-4f0f-c7a4-395063e26560"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 10975, 7875, 3270, 102], 'token_type_ids': [0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["tokenized_input"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x394QYEBy3P-","executionInfo":{"status":"ok","timestamp":1708160375154,"user_tz":-480,"elapsed":488,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"37f75103-a730-4c6b-cd7c-bfe19b7ba0fc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["### Apply this BERT tokenizer.convert_ids_to_tokens on 1 row for testing\n","- It converts integer to string/data, based on BERTS Tokenizer dictonary"],"metadata":{"id":"5_nftIrATJ_q"}},{"cell_type":"code","source":["tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"],"metadata":{"id":"sbCZhDpEyq1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAUZb2t3y9Lp","executionInfo":{"status":"ok","timestamp":1708153793973,"user_tz":-330,"elapsed":47,"user":{"displayName":"sunny savita","userId":"10135199589065204167"}},"outputId":"2dd95492-3ec3-4116-b46f-20affcf098bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['[CLS]',\n"," 'eu',\n"," 'rejects',\n"," 'german',\n"," 'call',\n"," 'to',\n"," 'boycott',\n"," 'british',\n"," 'lamb',\n"," '.',\n"," '[SEP]']"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["word_ids = tokenized_input.word_ids()\n","\n","print(word_ids)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GyOSDn4py-L0","executionInfo":{"status":"ok","timestamp":1708160643952,"user_tz":-480,"elapsed":514,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"66241335-0372-44b5-c98b-370d11879b2c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, None]\n"]}]},{"cell_type":"code","source":["example_text[\"ner_tags\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cfqg5K6z0sPD","executionInfo":{"status":"ok","timestamp":1708160645525,"user_tz":-480,"elapsed":10,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"a5c49a49-a5de-4dce-8b98-9c028f21df5c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[3, 0, 7, 0, 0, 0, 7, 0, 0]"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["# Add unique id to each lable/ner_tags\n","for i, label in enumerate(example_text[\"ner_tags\"]):\n","  print(i,label)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N02NWExd0zrn","executionInfo":{"status":"ok","timestamp":1708160684692,"user_tz":-480,"elapsed":496,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"3d43693b-f308-46ae-bc3f-c1fdb6d0924c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 3\n","1 0\n","2 7\n","3 0\n","4 0\n","5 0\n","6 7\n","7 0\n","8 0\n"]}]},{"cell_type":"markdown","source":["## Define Function **tokenize_and_align_labels**\n","-  Here this function converts token/word to integer/tokenized. It applies BERT'S tokenizers on whole data\n","- It adds Dummy NER to LABEL for starting CLS and ending SEP as -100\n","\n","- Labels will holds NER details of each token with starting CLS= -100 and ending SEP= -100, iIts needed because tokenizer adds these 2 extra after tokenization"],"metadata":{"id":"Sc8b93atT5i7"}},{"cell_type":"code","source":["def tokenize_and_align_labels(examples, label_all_tokens=True):\n","\n","    #tokeinze ids\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","    labels = []\n","\n","\n","    for i, label in enumerate(examples[\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)\n","        # word_ids() => Return a list mapping the tokens\n","        # to their actual word in the initial sentence.\n","        # It Returns a list indicating the word corresponding to each token.\n","\n","        previous_word_idx = None\n","        label_ids = []\n","        # Special tokens like `` and `<\\s>` are originally mapped to None\n","        # We need to set the label to -100 so they are automatically ignored in the loss function.\n","        for word_idx in word_ids:\n","            if word_idx is None:\n","                # set –100 as the label for these special tokens\n","                label_ids.append(-100)\n","\n","            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n","            # the label_all_tokens flag.\n","            elif word_idx != previous_word_idx:\n","                # if current word_idx is != prev then its the most regular case\n","                # and add the corresponding token\n","                label_ids.append(label[word_idx])\n","            else:\n","                # to take care of sub-words which have the same word_idx\n","                # set -100 as well for them, but only if label_all_tokens == False\n","                label_ids.append(label[word_idx] if label_all_tokens else -100)\n","                # mask the subword representations after the first subword\n","\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"],"metadata":{"id":"3ubJqVH1zN9k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["conll2003[\"train\"][4:5]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nhXC-akx1p6p","executionInfo":{"status":"ok","timestamp":1708160860934,"user_tz":-480,"elapsed":728,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"68e13c19-fa82-49c5-d117-aa5192dccdb6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': ['4'],\n"," 'tokens': [['Germany',\n","   \"'s\",\n","   'representative',\n","   'to',\n","   'the',\n","   'European',\n","   'Union',\n","   \"'s\",\n","   'veterinary',\n","   'committee',\n","   'Werner',\n","   'Zwingmann',\n","   'said',\n","   'on',\n","   'Wednesday',\n","   'consumers',\n","   'should',\n","   'buy',\n","   'sheepmeat',\n","   'from',\n","   'countries',\n","   'other',\n","   'than',\n","   'Britain',\n","   'until',\n","   'the',\n","   'scientific',\n","   'advice',\n","   'was',\n","   'clearer',\n","   '.']],\n"," 'pos_tags': [[22,\n","   27,\n","   21,\n","   35,\n","   12,\n","   22,\n","   22,\n","   27,\n","   16,\n","   21,\n","   22,\n","   22,\n","   38,\n","   15,\n","   22,\n","   24,\n","   20,\n","   37,\n","   21,\n","   15,\n","   24,\n","   16,\n","   15,\n","   22,\n","   15,\n","   12,\n","   16,\n","   21,\n","   38,\n","   17,\n","   7]],\n"," 'chunk_tags': [[11,\n","   11,\n","   12,\n","   13,\n","   11,\n","   12,\n","   12,\n","   11,\n","   12,\n","   12,\n","   12,\n","   12,\n","   21,\n","   13,\n","   11,\n","   12,\n","   21,\n","   22,\n","   11,\n","   13,\n","   11,\n","   1,\n","   13,\n","   11,\n","   17,\n","   11,\n","   12,\n","   12,\n","   21,\n","   1,\n","   0]],\n"," 'ner_tags': [[5,\n","   0,\n","   0,\n","   0,\n","   0,\n","   3,\n","   4,\n","   0,\n","   0,\n","   0,\n","   1,\n","   2,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   5,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0,\n","   0]]}"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["q=tokenize_and_align_labels(conll2003[\"train\"][4:5])\n","q\n","# Here this function converts token/word to integer/tokenized\n","# Labels will holds NER details of each token with starting CLS= -100 and ending SEP= -100, iIts needed because tokenizer adds these 2 extra after tokenization"],"metadata":{"id":"CtIPEziA1szx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708161040585,"user_tz":-480,"elapsed":544,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"2f7f3e56-6160-4adf-8c18-336fff5c4e88"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'input_ids': [[101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["# Used convert_ids_to_tokens to convert token value to actual word and appended lables(NER) as --\n","for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n","    print(f\"{token:_<40} {label}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LHM4esZX2COy","executionInfo":{"status":"ok","timestamp":1708160919763,"user_tz":-480,"elapsed":13,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"605d77f8-0d5b-4241-9c48-2f5c3bc9f4e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[CLS]___________________________________ -100\n","germany_________________________________ 5\n","'_______________________________________ 0\n","s_______________________________________ 0\n","representative__________________________ 0\n","to______________________________________ 0\n","the_____________________________________ 0\n","european________________________________ 3\n","union___________________________________ 4\n","'_______________________________________ 0\n","s_______________________________________ 0\n","veterinary______________________________ 0\n","committee_______________________________ 0\n","werner__________________________________ 1\n","z_______________________________________ 2\n","##wing__________________________________ 2\n","##mann__________________________________ 2\n","said____________________________________ 0\n","on______________________________________ 0\n","wednesday_______________________________ 0\n","consumers_______________________________ 0\n","should__________________________________ 0\n","buy_____________________________________ 0\n","sheep___________________________________ 0\n","##me____________________________________ 0\n","##at____________________________________ 0\n","from____________________________________ 0\n","countries_______________________________ 0\n","other___________________________________ 0\n","than____________________________________ 0\n","britain_________________________________ 5\n","until___________________________________ 0\n","the_____________________________________ 0\n","scientific______________________________ 0\n","advice__________________________________ 0\n","was_____________________________________ 0\n","clearer_________________________________ 0\n","._______________________________________ 0\n","[SEP]___________________________________ -100\n"]}]},{"cell_type":"markdown","source":["### Applying above function on entire data\n","- Here we preprocess our dataset as expected in the format of pretrained models input"],"metadata":{"id":"GAAdP1sFU_RF"}},{"cell_type":"code","source":["## Applying on entire data\n","tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"],"metadata":{"id":"-fEdnsKH2LyK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets[\"train\"][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TNpnZMfF29im","executionInfo":{"status":"ok","timestamp":1708161410245,"user_tz":-480,"elapsed":509,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"1954e71b-bf18-4b44-930f-fdbe384cbfa2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n"," 'input_ids': [101,\n","  7327,\n","  19164,\n","  2446,\n","  2655,\n","  2000,\n","  17757,\n","  2329,\n","  12559,\n","  1012,\n","  102],\n"," 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n"," 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n"," 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["#Earlier Original data\n","conll2003[\"train\"][0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffkObMgxWyb5","executionInfo":{"status":"ok","timestamp":1708161473556,"user_tz":-480,"elapsed":8,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"0cb94c77-6abb-4bfc-b306-2ad87bc259b2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'id': '0',\n"," 'tokens': ['EU',\n","  'rejects',\n","  'German',\n","  'call',\n","  'to',\n","  'boycott',\n","  'British',\n","  'lamb',\n","  '.'],\n"," 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n"," 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n"," 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["#### **Conclusion**\n","- Here we can see that function - **tokenize_and_align_labels**  added **'input_ids'** (Tokenized value) and  **'labels'** (NER value with -100 at the end and beginning) on original data, This is the format we wanted to use the existing pretrained model to fine tune."],"metadata":{"id":"9gIbGcttXRCh"}},{"cell_type":"markdown","source":["## Initialize Pretrained Classification model from - **bert-base-uncased**  Using class **AutoModelForTokenClassification**"],"metadata":{"id":"T2PI_VEXX0Ou"}},{"cell_type":"code","source":["model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\",num_labels=9)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104,"referenced_widgets":["d2aa59a0138f49b8b8eb7e1d6bec6c12","532f06d941d744a593858b832ae1b07e","65bbc56e51274f01a5f5fbb73c71ca72","e76c85a6716a4c66bf9f5da6e0c9febb","cc56203c0c7f474b93c4b427ae17c802","029d6640b1ef4bda9236f3daaad29304","f94896de21bb48b2a8a63ff16c137a72","be7ade465599475f807bf76d4e89770d","15c122eeba0f46e4bd466a35d80bc468","d50d04afe9cb40eda678e1d5fdfc320f","1c442860c9ca403381bc877dcfba3fe0"]},"id":"l-eRkzw53Eoh","executionInfo":{"status":"ok","timestamp":1708161856484,"user_tz":-480,"elapsed":8609,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"34ce345e-d288-45f1-c468-3eb9dd89cf77"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2aa59a0138f49b8b8eb7e1d6bec6c12"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["### To fine tune above pretrained model use **TrainingArguments**\n","\n","- Inside this we define all the hyperparameter and ephoc=1"],"metadata":{"id":"aCUNDbF4YYXH"}},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer"],"metadata":{"id":"lMiMj7mK3x0u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Initialize this data_collator by using bert's tokenizer\n","data_collator=DataCollatorForTokenClassification(tokenizer)"],"metadata":{"id":"tVZBMp6Tvf8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","\"test-ner\", # This is fine tune model new name\n","evaluation_strategy = \"epoch\",\n","learning_rate=2e-5,\n","per_device_train_batch_size=16,\n","per_device_eval_batch_size=16,\n","num_train_epochs=1,\n","weight_decay=0.01\n",")"],"metadata":{"id":"pbVuoJjP4CUt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Define Evaluation matric logic - **compute_metrics**"],"metadata":{"id":"BWfHfo6zaMvO"}},{"cell_type":"code","source":["def compute_metrics(eval_preds):\n","    pred_logits, labels = eval_preds\n","\n","    pred_logits = np.argmax(pred_logits, axis=2)\n","    # the logits and the probabilities are in the same order,\n","    # so we don’t need to apply the softmax\n","\n","    # We remove all the values where the label is -100\n","    predictions = [\n","        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(pred_logits, labels)\n","    ]\n","\n","    true_labels = [\n","      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n","       for prediction, label in zip(pred_logits, labels)\n","   ]\n","    results = metric.compute(predictions=predictions, references=true_labels)\n","\n","    return {\n","          \"precision\": results[\"overall_precision\"],\n","          \"recall\": results[\"overall_recall\"],\n","          \"f1\": results[\"overall_f1\"],\n","          \"accuracy\": results[\"overall_accuracy\"],\n","  }"],"metadata":{"id":"tOJGdpsfaLZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Get the Label List\n","\n","- Which is used inside **compute_metrics** function"],"metadata":{"id":"C3DjJNcqchqS"}},{"cell_type":"code","source":["label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n","label_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XiWBm2pNceB9","executionInfo":{"status":"ok","timestamp":1708163223161,"user_tz":-480,"elapsed":5,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"81db2135-6904-49aa-bcd5-3c65c33bcb67"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["metric=datasets.load_metric(\"seqeval\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eRDqIBmz6F1z","executionInfo":{"status":"ok","timestamp":1708163042855,"user_tz":-480,"elapsed":1434,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"8e6d9c11-81f0-44af-8b95-c1d0bbade564"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/seqeval/seqeval.py\n","You can avoid this message in future by passing the argument `trust_remote_code=True`.\n","Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## Fine tune the model"],"metadata":{"id":"2QWgFDFxg7er"}},{"cell_type":"markdown","source":["### Trainer - > This will fine tune pretrained model with our preprocessed data and hyperparameter"],"metadata":{"id":"JDx9acJWbRmb"}},{"cell_type":"code","source":["#Define Trainer all hyperparameter with its dataset, model, tokenizer datacollector etc\n","\n","trainer=Trainer(\n","   model,  #Pretrained model initiated from AutoModelForTokenClassification\n","   args,  # All the hyper parameter\n","   train_dataset=tokenized_datasets[\"train\"], # Pass our preprocessed/tokenized train data\n","   eval_dataset=tokenized_datasets[\"validation\"], # Pass our preprocessed validation data\n","   data_collator=data_collator, # Define the tokenizer used\n","   tokenizer=tokenizer, # Define the tokenizer used\n","   compute_metrics=compute_metrics\n",")"],"metadata":{"id":"2XSqn0Sb7qtu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the model / Fine tune the model\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"id":"MwRQ2KPr8ThI","executionInfo":{"status":"ok","timestamp":1708163223161,"user_tz":-480,"elapsed":173577,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"a6799051-8071-4643-bf14-2732aebf7873"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='878' max='878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [878/878 02:52, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.052500</td>\n","      <td>0.061754</td>\n","      <td>0.921638</td>\n","      <td>0.936794</td>\n","      <td>0.929154</td>\n","      <td>0.983526</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=878, training_loss=0.051343681057384724, metrics={'train_runtime': 172.9966, 'train_samples_per_second': 81.163, 'train_steps_per_second': 5.075, 'total_flos': 342221911376202.0, 'train_loss': 0.051343681057384724, 'epoch': 1.0})"]},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["## Save the Fine tuned Model as **ner_model** and Tokenizer as **tokenizer** in local"],"metadata":{"id":"LuYN4OCyb2IH"}},{"cell_type":"code","source":["#Save the model\n","model.save_pretrained(\"ner_model\")"],"metadata":{"id":"6vuTSmBu8V6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the Tokenizer\n","tokenizer.save_pretrained(\"tokenizer\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0tA7MCe9MVs","executionInfo":{"status":"ok","timestamp":1708163576969,"user_tz":-480,"elapsed":867,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"43c48ef6-60a0-48b8-b205-4d398aafd2e4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('tokenizer/tokenizer_config.json',\n"," 'tokenizer/special_tokens_map.json',\n"," 'tokenizer/vocab.txt',\n"," 'tokenizer/added_tokens.json',\n"," 'tokenizer/tokenizer.json')"]},"metadata":{},"execution_count":72}]},{"cell_type":"markdown","source":["## Update config.py's **id2label** and **label2id**\n","- During Fine tune model save, it creates  config but wont have actual mapping of label, it will have label1,2,3 .., There include mapping of lables both way (number--> string and string to number)"],"metadata":{"id":"6gLmFlnQdMsC"}},{"cell_type":"code","source":["import json"],"metadata":{"id":"V0LfS320-Pfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config=json.load(open(\"/content/ner_model/config.json\"))\n","config"],"metadata":{"id":"hC0wBuQb-Rgr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708163637522,"user_tz":-480,"elapsed":562,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"dece1fac-38d3-4a4f-a632-c1843bab3c2d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'_name_or_path': 'bert-base-uncased',\n"," 'architectures': ['BertForTokenClassification'],\n"," 'attention_probs_dropout_prob': 0.1,\n"," 'classifier_dropout': None,\n"," 'gradient_checkpointing': False,\n"," 'hidden_act': 'gelu',\n"," 'hidden_dropout_prob': 0.1,\n"," 'hidden_size': 768,\n"," 'id2label': {'0': 'LABEL_0',\n","  '1': 'LABEL_1',\n","  '2': 'LABEL_2',\n","  '3': 'LABEL_3',\n","  '4': 'LABEL_4',\n","  '5': 'LABEL_5',\n","  '6': 'LABEL_6',\n","  '7': 'LABEL_7',\n","  '8': 'LABEL_8'},\n"," 'initializer_range': 0.02,\n"," 'intermediate_size': 3072,\n"," 'label2id': {'LABEL_0': 0,\n","  'LABEL_1': 1,\n","  'LABEL_2': 2,\n","  'LABEL_3': 3,\n","  'LABEL_4': 4,\n","  'LABEL_5': 5,\n","  'LABEL_6': 6,\n","  'LABEL_7': 7,\n","  'LABEL_8': 8},\n"," 'layer_norm_eps': 1e-12,\n"," 'max_position_embeddings': 512,\n"," 'model_type': 'bert',\n"," 'num_attention_heads': 12,\n"," 'num_hidden_layers': 12,\n"," 'pad_token_id': 0,\n"," 'position_embedding_type': 'absolute',\n"," 'torch_dtype': 'float32',\n"," 'transformers_version': '4.35.2',\n"," 'type_vocab_size': 2,\n"," 'use_cache': True,\n"," 'vocab_size': 30522}"]},"metadata":{},"execution_count":81}]},{"cell_type":"code","source":["label_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3vgMNcqduaD","executionInfo":{"status":"ok","timestamp":1708163606739,"user_tz":-480,"elapsed":12,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"dbb5463f-2078-4352-e86d-e37a500d541c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["## Map the model output (Number) --> Text\n","id2label = {\n","    str(i): label for i,label in enumerate(label_list)\n","}\n","id2label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRNurFrMdxKy","executionInfo":{"status":"ok","timestamp":1708163610083,"user_tz":-480,"elapsed":11,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"84f15e1f-84e1-4c39-8edc-873ca9169b1c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'0': 'O',\n"," '1': 'B-PER',\n"," '2': 'I-PER',\n"," '3': 'B-ORG',\n"," '4': 'I-ORG',\n"," '5': 'B-LOC',\n"," '6': 'I-LOC',\n"," '7': 'B-MISC',\n"," '8': 'I-MISC'}"]},"metadata":{},"execution_count":77}]},{"cell_type":"code","source":["## Map the  Text -->model output\n","label2id = {\n","    label: str(i) for i,label in enumerate(label_list)\n","}\n","label2id"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uz2AdsxxdxxB","executionInfo":{"status":"ok","timestamp":1708163619284,"user_tz":-480,"elapsed":501,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"be3f8d52-1708-4743-89a2-429115c032d7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'O': '0',\n"," 'B-PER': '1',\n"," 'I-PER': '2',\n"," 'B-ORG': '3',\n"," 'I-ORG': '4',\n"," 'B-LOC': '5',\n"," 'I-LOC': '6',\n"," 'B-MISC': '7',\n"," 'I-MISC': '8'}"]},"metadata":{},"execution_count":78}]},{"cell_type":"code","source":["config[\"id2label\"] = id2label"],"metadata":{"id":"oqWpyHoh-cKS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config[\"label2id\"] = label2id"],"metadata":{"id":"TBoL387O-cNX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7tvZOYp5fNkC","executionInfo":{"status":"ok","timestamp":1708163674343,"user_tz":-480,"elapsed":479,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"c0041ec4-f3e8-4d1f-ae9b-3324f83f898f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'_name_or_path': 'bert-base-uncased',\n"," 'architectures': ['BertForTokenClassification'],\n"," 'attention_probs_dropout_prob': 0.1,\n"," 'classifier_dropout': None,\n"," 'gradient_checkpointing': False,\n"," 'hidden_act': 'gelu',\n"," 'hidden_dropout_prob': 0.1,\n"," 'hidden_size': 768,\n"," 'id2label': {'0': 'O',\n","  '1': 'B-PER',\n","  '2': 'I-PER',\n","  '3': 'B-ORG',\n","  '4': 'I-ORG',\n","  '5': 'B-LOC',\n","  '6': 'I-LOC',\n","  '7': 'B-MISC',\n","  '8': 'I-MISC'},\n"," 'initializer_range': 0.02,\n"," 'intermediate_size': 3072,\n"," 'label2id': {'O': '0',\n","  'B-PER': '1',\n","  'I-PER': '2',\n","  'B-ORG': '3',\n","  'I-ORG': '4',\n","  'B-LOC': '5',\n","  'I-LOC': '6',\n","  'B-MISC': '7',\n","  'I-MISC': '8'},\n"," 'layer_norm_eps': 1e-12,\n"," 'max_position_embeddings': 512,\n"," 'model_type': 'bert',\n"," 'num_attention_heads': 12,\n"," 'num_hidden_layers': 12,\n"," 'pad_token_id': 0,\n"," 'position_embedding_type': 'absolute',\n"," 'torch_dtype': 'float32',\n"," 'transformers_version': '4.35.2',\n"," 'type_vocab_size': 2,\n"," 'use_cache': True,\n"," 'vocab_size': 30522}"]},"metadata":{},"execution_count":86}]},{"cell_type":"code","source":["#Update config.py's id2label and label2id\n","json.dump(config,open(\"/content/ner_model/config.json\",\"w\"))"],"metadata":{"id":"nhT4qrDP-cS2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Invoke/Call fine tuned new model - **ner_model** from Local"],"metadata":{"id":"YHlpFrMHePjb"}},{"cell_type":"code","source":["model_fine_tuned=AutoModelForTokenClassification.from_pretrained(\"ner_model\")"],"metadata":{"id":"U1oEoLWS-cUo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_fine_tuned"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ko-y7l0C_F73","executionInfo":{"status":"ok","timestamp":1708163686418,"user_tz":-480,"elapsed":493,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"cf749b54-0158-449c-87c5-da5e1bed059a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",")"]},"metadata":{},"execution_count":89}]},{"cell_type":"markdown","source":["## Use **transformer pipeline** to do inference of fine tuned model"],"metadata":{"id":"fQLAZsxW9jT1"}},{"cell_type":"code","source":["from transformers import pipeline"],"metadata":{"id":"Ay6rKFif9j67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Initialize the pipeline\n","nlp_pipeline=pipeline(\"ner\", # Task we need to define for pipeline, this is ner task now\n","                      model=model_fine_tuned, #Fine tuned model\n","                      tokenizer=tokenizer #Tokenizer of that fine tuned model\n","                      )"],"metadata":{"id":"d7kMV1wQ9na-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nlp_pipeline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uoqS99LO_fy3","executionInfo":{"status":"ok","timestamp":1708163792559,"user_tz":-480,"elapsed":5,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"9509c7ba-9f67-4daf-d6b8-eb09524ac957"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<transformers.pipelines.token_classification.TokenClassificationPipeline at 0x7dc4f79ff130>"]},"metadata":{},"execution_count":92}]},{"cell_type":"code","source":["example=\"sudhanshu kumar is a foundar of iNeuron\"\n","nlp_pipeline(example)"],"metadata":{"id":"tYbT5QgDANPq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708163828712,"user_tz":-480,"elapsed":490,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"164453ea-0af1-466e-a8db-cc4ddd657d41"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-PER',\n","  'score': 0.99745697,\n","  'index': 1,\n","  'word': 'sud',\n","  'start': 0,\n","  'end': 3},\n"," {'entity': 'B-PER',\n","  'score': 0.9974349,\n","  'index': 2,\n","  'word': '##han',\n","  'start': 3,\n","  'end': 6},\n"," {'entity': 'B-PER',\n","  'score': 0.9977197,\n","  'index': 3,\n","  'word': '##shu',\n","  'start': 6,\n","  'end': 9},\n"," {'entity': 'I-PER',\n","  'score': 0.99450976,\n","  'index': 4,\n","  'word': 'kumar',\n","  'start': 10,\n","  'end': 15},\n"," {'entity': 'B-ORG',\n","  'score': 0.9819257,\n","  'index': 10,\n","  'word': 'in',\n","  'start': 32,\n","  'end': 34},\n"," {'entity': 'B-ORG',\n","  'score': 0.9871458,\n","  'index': 11,\n","  'word': '##eur',\n","  'start': 34,\n","  'end': 37},\n"," {'entity': 'B-ORG',\n","  'score': 0.98911977,\n","  'index': 12,\n","  'word': '##on',\n","  'start': 37,\n","  'end': 39}]"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["example=\"sunny is a founder of microsoft\"\n","nlp_pipeline(example)"],"metadata":{"id":"436NtdA-ArZZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708163838946,"user_tz":-480,"elapsed":527,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"4c9f6228-9977-4f93-e1a5-67b8038d9c13"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-PER',\n","  'score': 0.99303186,\n","  'index': 1,\n","  'word': 'sunny',\n","  'start': 0,\n","  'end': 5},\n"," {'entity': 'B-ORG',\n","  'score': 0.94221765,\n","  'index': 6,\n","  'word': 'microsoft',\n","  'start': 22,\n","  'end': 31}]"]},"metadata":{},"execution_count":98}]},{"cell_type":"code","source":["example=\"apple launch mobile while eating apple which taste like orange\"\n","nlp_pipeline(example)"],"metadata":{"id":"0h-wIi2hAzSg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708163847544,"user_tz":-480,"elapsed":14,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"829659e6-949d-4212-a276-c14ba44b4e4f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-ORG',\n","  'score': 0.9346753,\n","  'index': 1,\n","  'word': 'apple',\n","  'start': 0,\n","  'end': 5},\n"," {'entity': 'B-MISC',\n","  'score': 0.8264191,\n","  'index': 6,\n","  'word': 'apple',\n","  'start': 33,\n","  'end': 38}]"]},"metadata":{},"execution_count":99}]},{"cell_type":"code","source":["example=\"vikas is working ai engineer in google\"\n","nlp_pipeline(example)"],"metadata":{"id":"ISyq3QOwBHQg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708163854810,"user_tz":-480,"elapsed":494,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"caaf2198-b118-420e-b7f2-f76454366107"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-PER',\n","  'score': 0.9960549,\n","  'index': 1,\n","  'word': 'vi',\n","  'start': 0,\n","  'end': 2},\n"," {'entity': 'B-PER',\n","  'score': 0.9959552,\n","  'index': 2,\n","  'word': '##kas',\n","  'start': 2,\n","  'end': 5},\n"," {'entity': 'B-ORG',\n","  'score': 0.9780191,\n","  'index': 8,\n","  'word': 'google',\n","  'start': 32,\n","  'end': 38}]"]},"metadata":{},"execution_count":100}]},{"cell_type":"code","source":["example=\"apple founder loves eating apple\"\n","nlp_pipeline(example)"],"metadata":{"id":"fscsPD8eBTjT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708163863550,"user_tz":-480,"elapsed":481,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"87bdff38-14a2-448b-d337-d583df68f60d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-ORG',\n","  'score': 0.9772743,\n","  'index': 1,\n","  'word': 'apple',\n","  'start': 0,\n","  'end': 5}]"]},"metadata":{},"execution_count":101}]},{"cell_type":"code","source":["example=\"Microsoft Windows created their software by idea that came from the window of the house\"\n","nlp_pipeline(example)"],"metadata":{"id":"mpKOmQtMBeVW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708163878367,"user_tz":-480,"elapsed":749,"user":{"displayName":"prabha Melady","userId":"18413778025838058766"}},"outputId":"4c1c32b4-bcc6-40f7-8f80-6287ce3b0622"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'entity': 'B-ORG',\n","  'score': 0.9837806,\n","  'index': 1,\n","  'word': 'microsoft',\n","  'start': 0,\n","  'end': 9},\n"," {'entity': 'I-ORG',\n","  'score': 0.9599731,\n","  'index': 2,\n","  'word': 'windows',\n","  'start': 10,\n","  'end': 17}]"]},"metadata":{},"execution_count":102}]},{"cell_type":"markdown","source":["# **END**"],"metadata":{"id":"u53K5zNngIhK"}}]}